{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Starting on the Advanced Analytics Workspace The Advanced Analytics Workspace portal is a great place to discover and connect to the available resources we'll be talking about here. We'll break down the standard tasks into three categories: Experimentation / Analysis Publishing Large scale production All are important, and we will address all of them, but we'll focus on the first two as these are most widely applicable. For Experiments Jupyter notebooks R , Python , and Julia Choose the CPU/RAM you need, big or small, to fit your analysis Share your workspace with your team, along with the data and notebooks within Learn More Desktops with ML-Workspace Notebooks are more easily shared than desktops, but we also have the ability to run a full desktop, with typical applications, right inside your browser. Learn More For Publishing R Shiny The platform is designed to host any kind of open source application you want. We have an R-Shiny server for hosting R-Shiny apps To create any an R-Shiny Dashboard, you just have to submit a GitHub pull request to our R-Dashboards GitHub repository . For Production If an experiment turns into a product, then one of the following may be needed: Kubeflow pipelines for high-volume/intensity work Automation pipelines Ask for help in production The Advanced Analytics Workspace support staff are happy to help with production oriented use cases, and we can probably save you lots of time. Don't be shy to ask us for help ! How do I get data? How do I submit data? Every workspace can be equipped with its own storage. There are also storage buckets to publish datasets; either for internal use or for wider release. We will give an overview of the technologies here, and in the next sections there will be a more in-depth description of each of them. Browse some datasets Browse some datasets here. These data sets are meant to store widely shared data. Either data that has been brought it, or data to be released out as a product. As always, ensure that the data is not sensitive.","title":"Getting Started"},{"location":"#starting-on-the-advanced-analytics-workspace","text":"The Advanced Analytics Workspace portal is a great place to discover and connect to the available resources we'll be talking about here. We'll break down the standard tasks into three categories: Experimentation / Analysis Publishing Large scale production All are important, and we will address all of them, but we'll focus on the first two as these are most widely applicable.","title":"Starting on the Advanced Analytics Workspace"},{"location":"#for-experiments","text":"","title":"For Experiments"},{"location":"#jupyter-notebooks","text":"R , Python , and Julia Choose the CPU/RAM you need, big or small, to fit your analysis Share your workspace with your team, along with the data and notebooks within Learn More","title":"Jupyter notebooks"},{"location":"#desktops-with-ml-workspace","text":"Notebooks are more easily shared than desktops, but we also have the ability to run a full desktop, with typical applications, right inside your browser. Learn More","title":"Desktops with ML-Workspace"},{"location":"#for-publishing","text":"","title":"For Publishing"},{"location":"#r-shiny","text":"The platform is designed to host any kind of open source application you want. We have an R-Shiny server for hosting R-Shiny apps To create any an R-Shiny Dashboard, you just have to submit a GitHub pull request to our R-Dashboards GitHub repository .","title":"R Shiny"},{"location":"#for-production","text":"If an experiment turns into a product, then one of the following may be needed: Kubeflow pipelines for high-volume/intensity work Automation pipelines Ask for help in production The Advanced Analytics Workspace support staff are happy to help with production oriented use cases, and we can probably save you lots of time. Don't be shy to ask us for help !","title":"For Production"},{"location":"#how-do-i-get-data-how-do-i-submit-data","text":"Every workspace can be equipped with its own storage. There are also storage buckets to publish datasets; either for internal use or for wider release. We will give an overview of the technologies here, and in the next sections there will be a more in-depth description of each of them. Browse some datasets Browse some datasets here. These data sets are meant to store widely shared data. Either data that has been brought it, or data to be released out as a product. As always, ensure that the data is not sensitive.","title":"How do I get data? How do I submit data?"},{"location":"Collaboration/","text":"Collaboration on the Advanced Analytics Workspace There are lots of ways to collaborate on the platform, and what's best for you depends on what you're sharing and how many people you want to share with . We can roughly break the shareable things into Data and Code , and we can share the scope of who you're sharing with No one vs. My Team vs. Everyone . This leads to the following table of options Private Team StatCan Code GitLab/GitHub or personal folder GitLab/GitHub or team folder GitLab/GitHub Data Personal folder or bucket Team folder or bucket Shared Bucket What is the difference between a bucket and a folder? Buckets are like Network Storage. See the Storage section section for more discussion of the differences between these two ideas. The way that Private vs. Team based access is configured is with namespaces . So we start by talking about Kubeflow and Kubeflow namespaces. Then, Data and Code are better handled with slightly different tools, so we discuss the two separately. With Data we discuss buckets and MinIO , and with Code we discuss git , GitLab , and GitHub . This motivates the structure of this page Team Based Collaboration (applicable to Code and Data) Sharing Code Sharing Data Team Based Collaboration What does Kubeflow do? Kubeflow runs your workspaces . You can have notebook servers (called Jupyter Servers), and in them you can create analyses in R and Python with interactive visuals. You can save and upload data, download it, and your team can work along side you. Requesting a namespace By default, everyone gets their own personal namespace, firstname-lastname . But if you want to create a namespace for a team, then there is a button to submit a request for a namespace on the portal. Click the \u22ee menu on the Kubeflow section of the portal . The namespace cannot have special characters other than hyphens The namespace name must only be lowercase letters a-z with dashes. Otherwise, the namespace will not be created. You will receive an email notification when the namespace is created. Once the shared namespace is created, you can access it the same as any other namespace you have through the Kubeflow UI, like shown below. You will then be able to manage the collaborators list through Kubeflow's Manage Contributors tab, where you can add your colleagues to the shared namespace. To switch namespaces, take a look at the top of your window, just to the right of the Kubeflow Logo. Shared Code Teams have two options (but you can combine both!): Share a workspace in Kubeflow The advantage of sharing inside Kubeflow is that it's more free-form and it works better for .ipynb files (Jupyter notebooks). This method also lets you share a compute environment, so you can share resources very easily. When you share a workspace, you share A Private and Shared bucket ( /team-name and /shared/team-name ) All notebook servers in the Kubeflow Namespace Share with git, using GitLab or GitHub The advantage of sharing with git is that it works with users across namespaces, and keeping code in git is a great way to manage large software projects. Don't forget to include a License! If your code is public, do not forget to keep with the Innovation Team's guidelines and use a proper License if your work is done for Statistics Canada. Recommendation: Combine both It's a great idea to always use git, and using git along with shared workspaces is a great way to combine ad hoc sharing (through files) while also keeping your code organized and tracked. Shared Storage Sharing with your team Once you have a shared namespace, you have two shared storage approaches Storage Option Benefits Shared Jupyter Servers/Workspaces More amenable to small files, notebooks, and little experiments. Shared Buckets (see Storage ) Better suited for use in pipelines, APIs, and for large files. To learn more about the technology behind these, check out the Storage section . Sharing with StatCan In addition to private buckets, or team-shared private buckets, you can also place your files in shared storage . Within all bucket storage options ( minimal , premium , pachyderm ), you have a private bucket, and a folder inside of the shared bucket. Take a look, for instance, at the link below: shared/blair-drummond/ Any logged in user can see these files and read them freely. Sharing with the world Ask about that one in our Slack channel . There are many ways to do this from the IT side, but it's important for it to go through proper processes, so this is not done in a \"self-serve\" way that the others are. That said, it is totally possible.","title":"Collaboration"},{"location":"Collaboration/#collaboration-on-the-advanced-analytics-workspace","text":"There are lots of ways to collaborate on the platform, and what's best for you depends on what you're sharing and how many people you want to share with . We can roughly break the shareable things into Data and Code , and we can share the scope of who you're sharing with No one vs. My Team vs. Everyone . This leads to the following table of options Private Team StatCan Code GitLab/GitHub or personal folder GitLab/GitHub or team folder GitLab/GitHub Data Personal folder or bucket Team folder or bucket Shared Bucket What is the difference between a bucket and a folder? Buckets are like Network Storage. See the Storage section section for more discussion of the differences between these two ideas. The way that Private vs. Team based access is configured is with namespaces . So we start by talking about Kubeflow and Kubeflow namespaces. Then, Data and Code are better handled with slightly different tools, so we discuss the two separately. With Data we discuss buckets and MinIO , and with Code we discuss git , GitLab , and GitHub . This motivates the structure of this page Team Based Collaboration (applicable to Code and Data) Sharing Code Sharing Data","title":"Collaboration on the Advanced Analytics Workspace"},{"location":"Collaboration/#team-based-collaboration","text":"","title":"Team Based Collaboration"},{"location":"Collaboration/#what-does-kubeflow-do","text":"Kubeflow runs your workspaces . You can have notebook servers (called Jupyter Servers), and in them you can create analyses in R and Python with interactive visuals. You can save and upload data, download it, and your team can work along side you.","title":"What does Kubeflow do?"},{"location":"Collaboration/#requesting-a-namespace","text":"By default, everyone gets their own personal namespace, firstname-lastname . But if you want to create a namespace for a team, then there is a button to submit a request for a namespace on the portal. Click the \u22ee menu on the Kubeflow section of the portal . The namespace cannot have special characters other than hyphens The namespace name must only be lowercase letters a-z with dashes. Otherwise, the namespace will not be created. You will receive an email notification when the namespace is created. Once the shared namespace is created, you can access it the same as any other namespace you have through the Kubeflow UI, like shown below. You will then be able to manage the collaborators list through Kubeflow's Manage Contributors tab, where you can add your colleagues to the shared namespace. To switch namespaces, take a look at the top of your window, just to the right of the Kubeflow Logo.","title":"Requesting a namespace"},{"location":"Collaboration/#shared-code","text":"Teams have two options (but you can combine both!):","title":"Shared Code"},{"location":"Collaboration/#share-a-workspace-in-kubeflow","text":"The advantage of sharing inside Kubeflow is that it's more free-form and it works better for .ipynb files (Jupyter notebooks). This method also lets you share a compute environment, so you can share resources very easily. When you share a workspace, you share A Private and Shared bucket ( /team-name and /shared/team-name ) All notebook servers in the Kubeflow Namespace","title":"Share a workspace in Kubeflow"},{"location":"Collaboration/#share-with-git-using-gitlab-or-github","text":"The advantage of sharing with git is that it works with users across namespaces, and keeping code in git is a great way to manage large software projects. Don't forget to include a License! If your code is public, do not forget to keep with the Innovation Team's guidelines and use a proper License if your work is done for Statistics Canada.","title":"Share with git, using GitLab or GitHub"},{"location":"Collaboration/#recommendation-combine-both","text":"It's a great idea to always use git, and using git along with shared workspaces is a great way to combine ad hoc sharing (through files) while also keeping your code organized and tracked.","title":"Recommendation: Combine both"},{"location":"Collaboration/#shared-storage","text":"","title":"Shared Storage"},{"location":"Collaboration/#sharing-with-your-team","text":"Once you have a shared namespace, you have two shared storage approaches Storage Option Benefits Shared Jupyter Servers/Workspaces More amenable to small files, notebooks, and little experiments. Shared Buckets (see Storage ) Better suited for use in pipelines, APIs, and for large files. To learn more about the technology behind these, check out the Storage section .","title":"Sharing with your team"},{"location":"Collaboration/#sharing-with-statcan","text":"In addition to private buckets, or team-shared private buckets, you can also place your files in shared storage . Within all bucket storage options ( minimal , premium , pachyderm ), you have a private bucket, and a folder inside of the shared bucket. Take a look, for instance, at the link below: shared/blair-drummond/ Any logged in user can see these files and read them freely.","title":"Sharing with StatCan"},{"location":"Collaboration/#sharing-with-the-world","text":"Ask about that one in our Slack channel . There are many ways to do this from the IT side, but it's important for it to go through proper processes, so this is not done in a \"self-serve\" way that the others are. That said, it is totally possible.","title":"Sharing with the world"},{"location":"Help/","text":"Have questions? Or feedback? Come join us on the Advanced Analytics Workspace Slack channel ! You will find a bunch of other users of the platform there who may be able to answer your questions, and some of the engineers will usually be present on the channel. You can ask questions and provide feedback there. Slack (en) We will also post notices there if there are updates or downtime. Video tutorials After you have joined our Slack community, go and check out the following tutorials: Platform official Community driven content GitHub Want to know even more about our platform? Find everything about it on our GitHub page. COVID-19 Advanced Analytics Workspace on GitHub","title":"Help/Contact"},{"location":"Help/#have-questions-or-feedback","text":"Come join us on the Advanced Analytics Workspace Slack channel ! You will find a bunch of other users of the platform there who may be able to answer your questions, and some of the engineers will usually be present on the channel. You can ask questions and provide feedback there. Slack (en) We will also post notices there if there are updates or downtime.","title":"Have questions? Or feedback?"},{"location":"Help/#video-tutorials","text":"After you have joined our Slack community, go and check out the following tutorials: Platform official Community driven content","title":"Video tutorials"},{"location":"Help/#github","text":"Want to know even more about our platform? Find everything about it on our GitHub page. COVID-19 Advanced Analytics Workspace on GitHub","title":"GitHub"},{"location":"Storage/","text":"Storage The platform has a range of different types of storage meant for a variety of use-cases, so this storage section applies whether you're experimenting, creating pipelines, or publishing. At the surface, there are two kinds of storage Disks (also called Volumes) Buckets (S3 or \"Blob\" storage) Disks Disks are the familiar hard drive (or SSD) style file systems! You use them directly in Kubeflow when you add workspace and data volumes to your notebook server. They are automatically mounted at the directory you choose, and serve as a reliable way to preserve your data. Even if you delete your server later, you can still remount your disks to a new one \u2013 by default they are never destroyed. This is a super simple way to store your data. And if you share a workspace with a team, your whole team can use the same server's disk just like a shared drive. Buckets Buckets are slightly more complicated, but they are good at three things: Large amounts of data Buckets can be huge: way bigger than hard drives. And they are fast. Sharing You can share files from a bucket by sharing a URL that you can get through a simple web interface. This is great for sharing data with people outside of your workspace. Programmatic Access Most importantly, it's much easier for pipelines and web browsers to access data from buckets than from a hard drive. So if you want to use pipelines, you basically have to configure them to work with a bucket. Bucket Storage We have three available types of bucket storage. Self-Serve: Minimal : By default, use this one. It is HDD backed storage. Premium : Use this if you need very high read/write speeds, like for training models on very large datasets. Publicly Available: Public (Read-Only) Self-Serve In any of the three self-serve options, you can create a personal bucket. To log in, simply use the OpenID option seen below. Once you are logged in, you are allowed to create a personal bucket with the format firstname-lastname . Sharing You can easily share individual files. Just use the \"share\" option for a specific file and you will be provided a link that you can send to a collaborator! Programmatic Access We are currently working on letting you access your bucket storage via a folder in your notebook, but in the meantime you can access it programmatically with the command line tool mc , or via S3 API calls in R or Python. Required Kubeflow configuration If you want to enable bucket storage for your notebook, select \"Inject credentials to access MinIO object storage\" from the Configurations menu when you create your server. Otherwise, your server won't know how to sign-in to your personal storage. See the example notebooks! There is a template provided for connecting in R , python , or by the command line, provided in jupyter-notebooks/self-serve-storage . You can copy-paste and edit these examples! They should suit most of your needs. Connecting with mc To connect, simply run the following (replace FULLNAME=blair-drummond with your actual firstname-lastname ) #!/bin/sh FULLNAME = blair-drummond # Get the credentials source /vault/secrets/minio-minimal-tenant1 # Add the storage under the alias \"minio-minimal\" mc config host add minio-minimal $MINIO_URL $MINIO_ACCESS_KEY $MINIO_SECRET_KEY # Create a bucket under your name # NOTE: You can *only* create buckets named with your FIRSTNAME-LASTNAME. Any # other name will be rejected. # Private bucket (\"mb\" = \"make bucket\") mc mb minio-minimal/ ${ FULLNAME } # Shared bucket mc mb minio-minimal/shared/ ${ FULLNAME } # There you go! Now you can copy over files or folders! [ -f test.txt ] || echo \"This is a test\" > test.txt mc cp test.txt minio-minimal/ ${ FULLNAME } /test.txt Now open the MinIO browser and you will see your test file there! You can use mc to copy files to/from the bucket. It is very fast. You can also use mc --help to see what other options you have, like mc ls minio-minimal/FIRSTNAME-LASTNAME/ to list the contents of your bucket. Other storage options To use one of our other storage options: pachyderm or premium , simply replace minimal in the above program with the type you need.","title":"Storage"},{"location":"Storage/#storage","text":"The platform has a range of different types of storage meant for a variety of use-cases, so this storage section applies whether you're experimenting, creating pipelines, or publishing. At the surface, there are two kinds of storage Disks (also called Volumes) Buckets (S3 or \"Blob\" storage)","title":"Storage"},{"location":"Storage/#disks","text":"Disks are the familiar hard drive (or SSD) style file systems! You use them directly in Kubeflow when you add workspace and data volumes to your notebook server. They are automatically mounted at the directory you choose, and serve as a reliable way to preserve your data. Even if you delete your server later, you can still remount your disks to a new one \u2013 by default they are never destroyed. This is a super simple way to store your data. And if you share a workspace with a team, your whole team can use the same server's disk just like a shared drive.","title":"Disks"},{"location":"Storage/#buckets","text":"Buckets are slightly more complicated, but they are good at three things: Large amounts of data Buckets can be huge: way bigger than hard drives. And they are fast. Sharing You can share files from a bucket by sharing a URL that you can get through a simple web interface. This is great for sharing data with people outside of your workspace. Programmatic Access Most importantly, it's much easier for pipelines and web browsers to access data from buckets than from a hard drive. So if you want to use pipelines, you basically have to configure them to work with a bucket.","title":"Buckets"},{"location":"Storage/#bucket-storage","text":"We have three available types of bucket storage. Self-Serve: Minimal : By default, use this one. It is HDD backed storage. Premium : Use this if you need very high read/write speeds, like for training models on very large datasets. Publicly Available: Public (Read-Only)","title":"Bucket Storage"},{"location":"Storage/#self-serve","text":"In any of the three self-serve options, you can create a personal bucket. To log in, simply use the OpenID option seen below. Once you are logged in, you are allowed to create a personal bucket with the format firstname-lastname .","title":"Self-Serve"},{"location":"Storage/#sharing","text":"You can easily share individual files. Just use the \"share\" option for a specific file and you will be provided a link that you can send to a collaborator!","title":"Sharing"},{"location":"Storage/#programmatic-access","text":"We are currently working on letting you access your bucket storage via a folder in your notebook, but in the meantime you can access it programmatically with the command line tool mc , or via S3 API calls in R or Python. Required Kubeflow configuration If you want to enable bucket storage for your notebook, select \"Inject credentials to access MinIO object storage\" from the Configurations menu when you create your server. Otherwise, your server won't know how to sign-in to your personal storage. See the example notebooks! There is a template provided for connecting in R , python , or by the command line, provided in jupyter-notebooks/self-serve-storage . You can copy-paste and edit these examples! They should suit most of your needs.","title":"Programmatic Access"},{"location":"Storage/#connecting-with-mc","text":"To connect, simply run the following (replace FULLNAME=blair-drummond with your actual firstname-lastname ) #!/bin/sh FULLNAME = blair-drummond # Get the credentials source /vault/secrets/minio-minimal-tenant1 # Add the storage under the alias \"minio-minimal\" mc config host add minio-minimal $MINIO_URL $MINIO_ACCESS_KEY $MINIO_SECRET_KEY # Create a bucket under your name # NOTE: You can *only* create buckets named with your FIRSTNAME-LASTNAME. Any # other name will be rejected. # Private bucket (\"mb\" = \"make bucket\") mc mb minio-minimal/ ${ FULLNAME } # Shared bucket mc mb minio-minimal/shared/ ${ FULLNAME } # There you go! Now you can copy over files or folders! [ -f test.txt ] || echo \"This is a test\" > test.txt mc cp test.txt minio-minimal/ ${ FULLNAME } /test.txt Now open the MinIO browser and you will see your test file there! You can use mc to copy files to/from the bucket. It is very fast. You can also use mc --help to see what other options you have, like mc ls minio-minimal/FIRSTNAME-LASTNAME/ to list the contents of your bucket. Other storage options To use one of our other storage options: pachyderm or premium , simply replace minimal in the above program with the type you need.","title":"Connecting with mc"},{"location":"1-Experiments/Databricks/","text":"Databricks","title":"DataBricks"},{"location":"1-Experiments/Databricks/#databricks","text":"","title":"Databricks"},{"location":"1-Experiments/Jupyter/","text":"Jupyter Friendly R and Python experience Jupyter gives you notebooks to write your code and make visualizations. You can quickly iterate, visualize, and share your analyses. Because it's running on a server (that you set up in the last section) you can do really big analyses on centralized hardware! Adding as much horsepower as you need! And because it's on the cloud, you can share it with your colleagues too. Explore your data Jupyter comes with a number of features (and we can add more) Integrated visuals within your notebook Data volume for storing your data You can share your workspace with colleagues. IDE in the browser Create for exploring, and also great for writing code Linting and a debugger Git integration Built in Terminal Light/Dark theme (change settings at the top) More information on Jupyter here Get started with the examples When you started your server, it got loaded with a bunch of example notebooks. Great notebooks to start with are R/01-R-Notebook-Demo.ipynb , or the notebooks in scikitlearn . pytorch and tensorflow are great if you are familiar with machine learning. The mapreduce-pipeline and ai-pipeline are more advanced. Some notebooks only work in certain server versions For instance, gdal is only in the geomatics image. So if you use another image then a notebook using gdal might not work. Adding software You do not have sudo in Jupyter, but you can use conda install --use-local your_package_name or pip install --user your_package_name Don't forget to restart your Jupyter kernel afterwards, to make new packages available. Make sure to restart the Jupyter kernel after installing new software If you install software in a terminal, but your Jupyter kernel was already running, then it will not be updated. Is there something that you can't install? If you need something installed, reach us or open a GitHub issue . We can add it to the default software. Getting Data in and out of Jupyter You can upload and download data to/from JupyterHub directly in the menu. There is an upload button at the top, and you can right-click most files or folders to download them. Shareable \"Bucket\" storage The other option is high-volume storage with Object Storage . Because storage is important for experiments, publishing, and exploring datasets, it has its own section. Refer to the Storage Section","title":"Jupyter"},{"location":"1-Experiments/Jupyter/#jupyter","text":"","title":"Jupyter"},{"location":"1-Experiments/Jupyter/#friendly-r-and-python-experience","text":"Jupyter gives you notebooks to write your code and make visualizations. You can quickly iterate, visualize, and share your analyses. Because it's running on a server (that you set up in the last section) you can do really big analyses on centralized hardware! Adding as much horsepower as you need! And because it's on the cloud, you can share it with your colleagues too.","title":"Friendly R and Python experience"},{"location":"1-Experiments/Jupyter/#explore-your-data","text":"Jupyter comes with a number of features (and we can add more) Integrated visuals within your notebook Data volume for storing your data You can share your workspace with colleagues.","title":"Explore your data"},{"location":"1-Experiments/Jupyter/#ide-in-the-browser","text":"Create for exploring, and also great for writing code Linting and a debugger Git integration Built in Terminal Light/Dark theme (change settings at the top) More information on Jupyter here","title":"IDE in the browser"},{"location":"1-Experiments/Jupyter/#get-started-with-the-examples","text":"When you started your server, it got loaded with a bunch of example notebooks. Great notebooks to start with are R/01-R-Notebook-Demo.ipynb , or the notebooks in scikitlearn . pytorch and tensorflow are great if you are familiar with machine learning. The mapreduce-pipeline and ai-pipeline are more advanced. Some notebooks only work in certain server versions For instance, gdal is only in the geomatics image. So if you use another image then a notebook using gdal might not work.","title":"Get started with the examples"},{"location":"1-Experiments/Jupyter/#adding-software","text":"You do not have sudo in Jupyter, but you can use conda install --use-local your_package_name or pip install --user your_package_name Don't forget to restart your Jupyter kernel afterwards, to make new packages available. Make sure to restart the Jupyter kernel after installing new software If you install software in a terminal, but your Jupyter kernel was already running, then it will not be updated. Is there something that you can't install? If you need something installed, reach us or open a GitHub issue . We can add it to the default software.","title":"Adding software"},{"location":"1-Experiments/Jupyter/#getting-data-in-and-out-of-jupyter","text":"You can upload and download data to/from JupyterHub directly in the menu. There is an upload button at the top, and you can right-click most files or folders to download them.","title":"Getting Data in and out of Jupyter"},{"location":"1-Experiments/Jupyter/#shareable-bucket-storage","text":"The other option is high-volume storage with Object Storage . Because storage is important for experiments, publishing, and exploring datasets, it has its own section. Refer to the Storage Section","title":"Shareable \"Bucket\" storage"},{"location":"1-Experiments/Kubeflow/","text":"Getting started with Kubeflow What does Kubeflow do? Kubeflow runs your workspaces . You can have notebook servers (called Jupyter Servers), and in them you can create analyses in R and Python with interactive visuals. You can save and upload data, download it, and create shared workspaces for your team. Let's get started! Create a Server Log into Kubeflow Log into the Azure portal using your cloud.statcan.ca credentials . Log into the Azure Portal using your Cloud Credentials You have to login to the azure portal using your StatCan credentials . first.lastname@cloud.statcan.ca . You can do that using the Azure portal . After logging into Azure, log into Kubeflow Why am I getting Missing url parameter: code ? If you try to log into Kubeflow and you get the message: Missing url parameter: code It is because you are signed in with the wrong Azure account. You must sign in with your cloud credentials. Navigate to the Jupyter Servers tab Then click + New Server Configuring your server You will get a template to create your notebook server. Note: the name must be lowercase letters with hyphens. No spaces, and no underscores. You'll need to choose an image You will probably want one of Machine Learning Geomatics Minimal If you want to use a GPU, check if the image says cpu or gpu . CPU and Memory At the time of writing (April 21, 2020) there are two types of computers in the cluster CPU: D16s v3 (16 CPU cores, 64 GiB memory) GPU: NC6s_v3 (6 CPU cores, 112 GiB memory, 1 GPU) Because of this, if you request too much RAM or too many CPUs, it may be hard or impossible to satisfy your request. In the future (possibly when you read this) there may be larger machines made available, so you may have looser restrictions. Use GPU machines responsibly There are fewer GPU machines than CPU machines, so use them responsibly. Storing your data You'll want to create a data volume! You'll be able to save your work here, and if you shut down your server, you'll be able to just remount your old data by entering the name of your old disk. It is important that you remember the volume's name. Check for old volumes by looking at the Existing option When you create your server you have the option of reusing an old volume or creating a new one. You probably want to reuse your old volume. And... Create!!! If you're satisfied with the settings, you can now create the server! It may take a few minutes to spin up depending on the resources you asked for. (GPUs take longer.) Your server is running If all goes well, your server should be running!!! You will now have the option to connect, and try out Jupyter! Share your workspace In Kubeflow every user has a namespace . Your namespace belongs to you, and it's where all your resources live. If you want to collaborate with someone you need to share a namespace. So you can do that either by sharing your own namespace, or more preferably, by creating a team namespace . The link to create a new namespace is in the \u22ee menu on the Kubeflow section of the portal . Manage contributors You can add or remove people from a namespace you already own through the Manage Contributors menu in Kubeflow. Now you and your colleagues can share access to a server! Now you can share a server with colleagues! Try it out! For more details on collaboration on the platform, see Collaboration .","title":"Kubeflow"},{"location":"1-Experiments/Kubeflow/#getting-started-with-kubeflow","text":"","title":"Getting started with Kubeflow"},{"location":"1-Experiments/Kubeflow/#what-does-kubeflow-do","text":"Kubeflow runs your workspaces . You can have notebook servers (called Jupyter Servers), and in them you can create analyses in R and Python with interactive visuals. You can save and upload data, download it, and create shared workspaces for your team. Let's get started!","title":"What does Kubeflow do?"},{"location":"1-Experiments/Kubeflow/#create-a-server","text":"","title":"Create a Server"},{"location":"1-Experiments/Kubeflow/#log-into-kubeflow","text":"Log into the Azure portal using your cloud.statcan.ca credentials . Log into the Azure Portal using your Cloud Credentials You have to login to the azure portal using your StatCan credentials . first.lastname@cloud.statcan.ca . You can do that using the Azure portal . After logging into Azure, log into Kubeflow Why am I getting Missing url parameter: code ? If you try to log into Kubeflow and you get the message: Missing url parameter: code It is because you are signed in with the wrong Azure account. You must sign in with your cloud credentials. Navigate to the Jupyter Servers tab Then click + New Server","title":"Log into Kubeflow"},{"location":"1-Experiments/Kubeflow/#configuring-your-server","text":"You will get a template to create your notebook server. Note: the name must be lowercase letters with hyphens. No spaces, and no underscores. You'll need to choose an image You will probably want one of Machine Learning Geomatics Minimal If you want to use a GPU, check if the image says cpu or gpu .","title":"Configuring your server"},{"location":"1-Experiments/Kubeflow/#cpu-and-memory","text":"At the time of writing (April 21, 2020) there are two types of computers in the cluster CPU: D16s v3 (16 CPU cores, 64 GiB memory) GPU: NC6s_v3 (6 CPU cores, 112 GiB memory, 1 GPU) Because of this, if you request too much RAM or too many CPUs, it may be hard or impossible to satisfy your request. In the future (possibly when you read this) there may be larger machines made available, so you may have looser restrictions. Use GPU machines responsibly There are fewer GPU machines than CPU machines, so use them responsibly.","title":"CPU and Memory"},{"location":"1-Experiments/Kubeflow/#storing-your-data","text":"You'll want to create a data volume! You'll be able to save your work here, and if you shut down your server, you'll be able to just remount your old data by entering the name of your old disk. It is important that you remember the volume's name. Check for old volumes by looking at the Existing option When you create your server you have the option of reusing an old volume or creating a new one. You probably want to reuse your old volume.","title":"Storing your data"},{"location":"1-Experiments/Kubeflow/#and-create","text":"If you're satisfied with the settings, you can now create the server! It may take a few minutes to spin up depending on the resources you asked for. (GPUs take longer.) Your server is running If all goes well, your server should be running!!! You will now have the option to connect, and try out Jupyter!","title":"And... Create!!!"},{"location":"1-Experiments/Kubeflow/#share-your-workspace","text":"In Kubeflow every user has a namespace . Your namespace belongs to you, and it's where all your resources live. If you want to collaborate with someone you need to share a namespace. So you can do that either by sharing your own namespace, or more preferably, by creating a team namespace . The link to create a new namespace is in the \u22ee menu on the Kubeflow section of the portal .","title":"Share your workspace"},{"location":"1-Experiments/Kubeflow/#manage-contributors","text":"You can add or remove people from a namespace you already own through the Manage Contributors menu in Kubeflow. Now you and your colleagues can share access to a server! Now you can share a server with colleagues! Try it out! For more details on collaboration on the platform, see Collaboration .","title":"Manage contributors"},{"location":"1-Experiments/MLflow/","text":"MLflow for model tracking","title":"MLflow"},{"location":"1-Experiments/MLflow/#mlflow-for-model-tracking","text":"","title":"MLflow for model tracking"},{"location":"1-Experiments/Remote-Desktop/","text":"Remote Desktop What is Remote Desktop? Remote Desktop provides an in-browser GUI Ubuntu desktop experience as well as quick access to supporting tools. The operating system is Ubuntu 18.04 with the XFCE desktop environment. Versions Two versions of Remote Desktop are available. R includes R and RStudio. Geomatics extends R with QGIS and various supporting libraries. You may further customize your Remote Desktop workspace to suit your specific needs and preferences. Customization pip , conda , npm and yarn are available to install various packages. Accessing the Remote Desktop To launch the Remote Desktop or any of its supporting tools, create a Notebook Server in Kubeflow and select one of the available versions in the image dropdown. Then, click Connect to access the landing page. Remote Desktop brings you to the Desktop GUI through a noVNC session. Click on the < on the left side of the screen to expand a panel with options such as fullscreen and clipboard access. Legacy Interface: Jupyter Notebook Jupyter Notebook is a legacy interface for managing Jupyter Notebooks until storage integration with the JupyterLab images becomes available. You can access other tools and interfaces with the Open Tool button in the top right corner, or by returning to the landing page. In-browser Tools VS Code VS Code brings you to the Visual Studio Code IDE. Netdata Netdata delivers in-depth interactive resource monitoring. Charts can be panned by dragging them. You can also zoom in and out with SHIFT + mouse wheel , or zoom to a selection by holding down SHIFT while dragging. Filebrowser Filebrowser can be used to quickly explore the file system of the Remote Desktop as well as to transfer files between the Remote Desktop and your computer. Access Port Finally, Access Port provides in-browser access to anything served on a specified port inside the Remote Desktop (internally accessible at e.g. localhost:8059 ). As with the other tools, this opens in a separate page in your browser. Your namespace collaborators can also access this page if you send them the URL. Example: Accessing the Supervisor API at port 8059 Footnotes Remote Desktop is based on ml-tooling/ml-workspace .","title":"Remote Desktop"},{"location":"1-Experiments/Remote-Desktop/#remote-desktop","text":"","title":"Remote Desktop"},{"location":"1-Experiments/Remote-Desktop/#what-is-remote-desktop","text":"Remote Desktop provides an in-browser GUI Ubuntu desktop experience as well as quick access to supporting tools. The operating system is Ubuntu 18.04 with the XFCE desktop environment.","title":"What is Remote Desktop?"},{"location":"1-Experiments/Remote-Desktop/#versions","text":"Two versions of Remote Desktop are available. R includes R and RStudio. Geomatics extends R with QGIS and various supporting libraries. You may further customize your Remote Desktop workspace to suit your specific needs and preferences.","title":"Versions"},{"location":"1-Experiments/Remote-Desktop/#customization","text":"pip , conda , npm and yarn are available to install various packages.","title":"Customization"},{"location":"1-Experiments/Remote-Desktop/#accessing-the-remote-desktop","text":"To launch the Remote Desktop or any of its supporting tools, create a Notebook Server in Kubeflow and select one of the available versions in the image dropdown. Then, click Connect to access the landing page. Remote Desktop brings you to the Desktop GUI through a noVNC session. Click on the < on the left side of the screen to expand a panel with options such as fullscreen and clipboard access.","title":"Accessing the Remote Desktop"},{"location":"1-Experiments/Remote-Desktop/#legacy-interface-jupyter-notebook","text":"Jupyter Notebook is a legacy interface for managing Jupyter Notebooks until storage integration with the JupyterLab images becomes available. You can access other tools and interfaces with the Open Tool button in the top right corner, or by returning to the landing page.","title":"Legacy Interface: Jupyter Notebook"},{"location":"1-Experiments/Remote-Desktop/#in-browser-tools","text":"","title":"In-browser Tools"},{"location":"1-Experiments/Remote-Desktop/#vs-code","text":"VS Code brings you to the Visual Studio Code IDE.","title":"VS Code"},{"location":"1-Experiments/Remote-Desktop/#netdata","text":"Netdata delivers in-depth interactive resource monitoring. Charts can be panned by dragging them. You can also zoom in and out with SHIFT + mouse wheel , or zoom to a selection by holding down SHIFT while dragging.","title":"Netdata"},{"location":"1-Experiments/Remote-Desktop/#filebrowser","text":"Filebrowser can be used to quickly explore the file system of the Remote Desktop as well as to transfer files between the Remote Desktop and your computer.","title":"Filebrowser"},{"location":"1-Experiments/Remote-Desktop/#access-port","text":"Finally, Access Port provides in-browser access to anything served on a specified port inside the Remote Desktop (internally accessible at e.g. localhost:8059 ). As with the other tools, this opens in a separate page in your browser. Your namespace collaborators can also access this page if you send them the URL. Example: Accessing the Supervisor API at port 8059","title":"Access Port"},{"location":"1-Experiments/Remote-Desktop/#footnotes","text":"Remote Desktop is based on ml-tooling/ml-workspace .","title":"Footnotes"},{"location":"2-Publishing/Custom/","text":"Custom Web Apps We can deploy anything as long as it's open source and we can put it in a Docker container. For instance, Node.js apps, Flask or Dash apps. Etc. See the source code for this app We just push these kinds of applications through GitHub into the server. The source for the above app is StatCan/covid19 How to get your app hosted If you already have a web app in a git repository then, as soon as it's containerized, we can fork the Git repository into the StatCan GitHub repository and point a URL to it. To update it, you'll just interact with the StatCan GitHub repository with Pull Requests. Contact us if you have questions.","title":"Custom"},{"location":"2-Publishing/Custom/#custom-web-apps","text":"We can deploy anything as long as it's open source and we can put it in a Docker container. For instance, Node.js apps, Flask or Dash apps. Etc. See the source code for this app We just push these kinds of applications through GitHub into the server. The source for the above app is StatCan/covid19","title":"Custom Web Apps"},{"location":"2-Publishing/Custom/#how-to-get-your-app-hosted","text":"If you already have a web app in a git repository then, as soon as it's containerized, we can fork the Git repository into the StatCan GitHub repository and point a URL to it. To update it, you'll just interact with the StatCan GitHub repository with Pull Requests. Contact us if you have questions.","title":"How to get your app hosted"},{"location":"2-Publishing/PowerBI/","text":"Loading data into Power BI We do not offer a Power BI server, but you can pull your data into Power BI from our Storage system, and use the data as a pandas data frame. What you'll need A computer with Power BI, and Python 3.6 Your MinIO ACCESS_KEY and SECRET_KEY on hand. (See Storage ) Get connected Set up Power BI Open up your Power BI system, and open up this Power BI quick start in your favourite text editor. You'll have to make sure that pandas , boto3 , and numpy are installed, and that you're using the right Conda virtual environment (if applicable). You'll then need to make sure that Power BI is using the correct Python environment. This is modified from the options menu, and the exact path is specified in the quick start guide. Edit your python script Then, edit your Python script to use your MinIO ACCESS_KEY and SECRET_KEY , and then click \"Get Data\" and copy it in as a Python Script.","title":"Power BI"},{"location":"2-Publishing/PowerBI/#loading-data-into-power-bi","text":"We do not offer a Power BI server, but you can pull your data into Power BI from our Storage system, and use the data as a pandas data frame.","title":"Loading data into Power BI"},{"location":"2-Publishing/PowerBI/#what-youll-need","text":"A computer with Power BI, and Python 3.6 Your MinIO ACCESS_KEY and SECRET_KEY on hand. (See Storage )","title":"What you'll need"},{"location":"2-Publishing/PowerBI/#get-connected","text":"","title":"Get connected"},{"location":"2-Publishing/PowerBI/#set-up-power-bi","text":"Open up your Power BI system, and open up this Power BI quick start in your favourite text editor. You'll have to make sure that pandas , boto3 , and numpy are installed, and that you're using the right Conda virtual environment (if applicable). You'll then need to make sure that Power BI is using the correct Python environment. This is modified from the options menu, and the exact path is specified in the quick start guide.","title":"Set up Power BI"},{"location":"2-Publishing/PowerBI/#edit-your-python-script","text":"Then, edit your Python script to use your MinIO ACCESS_KEY and SECRET_KEY , and then click \"Get Data\" and copy it in as a Python Script.","title":"Edit your python script"},{"location":"2-Publishing/R-Shiny/","text":"Deploying your R-Shiny dashboard! We handle the R-Shiny server, and it's super easy to get your dashboard onto the platform. Just send a pull request! All you have to do is send a pull request to our R-Dashboards repository . Include your repository in a folder with the name you want (for example, \"air-quality-dashboard\"). Then we will approve it and it will come online. If you need extra R libraries to be installed, send your list to the R-Shiny repository by creating a GitHub Issue and we will add the dependencies. See the above dashboard here The above dashboard is in GitHub. Take a look at the source , and see the dashboard live . Embedding dashboards into your websites Embedding dashboards in other sites We have not had a chance to look at this or prototype it yet, but if you have a use-case, feel free to reach out to engineering. We will work with you to figure something out.","title":"R Shiny"},{"location":"2-Publishing/R-Shiny/#deploying-your-r-shiny-dashboard","text":"We handle the R-Shiny server, and it's super easy to get your dashboard onto the platform.","title":"Deploying your R-Shiny dashboard!"},{"location":"2-Publishing/R-Shiny/#just-send-a-pull-request","text":"All you have to do is send a pull request to our R-Dashboards repository . Include your repository in a folder with the name you want (for example, \"air-quality-dashboard\"). Then we will approve it and it will come online. If you need extra R libraries to be installed, send your list to the R-Shiny repository by creating a GitHub Issue and we will add the dependencies. See the above dashboard here The above dashboard is in GitHub. Take a look at the source , and see the dashboard live .","title":"Just send a pull request!"},{"location":"2-Publishing/R-Shiny/#embedding-dashboards-into-your-websites","text":"Embedding dashboards in other sites We have not had a chance to look at this or prototype it yet, but if you have a use-case, feel free to reach out to engineering. We will work with you to figure something out.","title":"Embedding dashboards into your websites"},{"location":"3-Pipelines/Kubeflow-Pipelines/","text":"Overview Kubeflow Pipelines is a platform for building machine learning workflows for deployment in a Kubernetes environment. It enables authoring pipelines that encapsulate analytical workflows (transforming data, training models, building visuals, etc.). These pipelines can be shared, reused, and scheduled, and are built to run on compute provided via Kubernetes. Here is an example of a pipeline with many sample steps feeding into a single average step. This image comes from the Kubeflow Pipelines UI In the context of the Advanced Analytics Workspace, Kubeflow Pipelines are interacted with through: The Kubeflow UI , where from the Pipelines menu you can upload pipelines, view the pipelines you have and their results, etc. The Kubeflow Pipelines python SDK , accessible through the Jupyter Notebook Servers , where you can define your components and pipelines, submit them to run now, or even save them for later. More examples in the notebooks More comprehensive pipeline examples specifically made for this platform are available on GitHub (and in every Notebook Server at /jupyter-notebooks ). You can also check out public sources . See the official Kubeflow docs for a more detailed explanation of Kubeflow Pipelines. What are pipelines and how do they work? A pipeline in Kubeflow Pipelines consists of one or more pipeline components chained together to form a workflow. The components are like functions, describing the individual steps in your workflow (such as pulling columns from a data store, transforming data, or training a model). The pipeline is the logic that glues components together, such as: Run Component-A Pass the output from Component-A to Component-B and Component-C ... In the above image, the logic would be running many sample steps followed by a single average step. At their core, each component has: A standalone application, packaged as a Docker image , for doing the actual work. The code in the Docker image could be a shell script, Python script, or anything else you can run from a Linux terminal, and generally will have a command line interface for data exchange (accessible through docker run ) A YAML file that describes how Kubeflow Pipelines runs this code (what Docker image should be run, what command line arguments does it accept, what output does it generate) Each component should be single purpose , modular , and reusable . Define and run your first pipeline using the Python SDK While pipelines and components are defined in Kubeflow Pipelines by YAML files that use Docker images, that does not mean we have to work directly with either YAML files or Docker images. The Kubeflow Pipelines SDK provides a way for us to define our pipeline and components directly in Python code, where the SDK then translates our Python code to YAML files for us. For our first example, let's define a simple pipeline using only the Python SDK. The purpose of this section is to give a high level view of component and pipeline authoring, not a deep dive. More detailed looks into defining your own components , passing data between components , and returning data from your pipeline are explained in more detail in further sections. The demo pipeline we define will do the following: Accept five numbers as arguments Average of the first three numbers Average of the last two numbers Average of the results of (2) and (3) To do this, we will first define our component . Our average component will call a Docker image that does the following: Accepts one or more numbers as command line arguments Returns the average of these numbers by writing them to an output file in the container (by default, to out.txt ) This Docker image is already built for us and stored in our container registry here: k8scc01covidacr.azurecr.io/kfp-components/average:v1 . Don't worry if you don't know Docker - since the image is built already, we only have to tell Kubeflow Pipelines where it is. ??? info \"Full details of the average component's Docker image are in GitHub \" This image effectively runs the following code (slightly cleaned up for brevity). By making average.py accept an arbitrary set of numbers as inputs, we can use the same average component for all steps in our pipeline : import argparse def parse_args (): parser = argparse . ArgumentParser ( description = \"Returns the average of one or \" \"more numbers as a JSON file\" ) parser . add_argument ( \"numbers\" , type = float , nargs = \"+\" , help = \"One or more numbers\" ) parser . add_argument ( \"--output_file\" , type = str , default = \"out.txt\" , help = \"Filename \" \"to write output number to\" ) return parser . parse_args () if __name__ == '__main__' : args = parse_args () numbers = args . numbers output_file = args . output_file print ( f \"Averaging numbers: {numbers}\" ) avg = sum ( numbers ) / len ( numbers ) print ( f \"Result = {avg}\" ) print ( f \"Writing output to {output_file}\" ) with open ( output_file , 'w' ) as fout : fout . write ( str ( avg )) print ( \"Done\" ) To make our average image into a Kubeflow Pipelines component , we make a kfp.dsl.ContainerOp in Python that defines how Kubeflow Pipelines interacts with our container, specifying: The Docker image location to use How to pass arguments to the running container What outputs to expect from the container We could use ContainerOp directly, but since we'll use average a few times we instead create a factory function we can reuse: from kfp import dsl def average_op ( * numbers ): \"\"\" Factory for average ContainerOps Accepts an arbitrary number of input numbers, returning a ContainerOp that passes those numbers to the underlying Docker image for averaging Returns output collected from ./out.txt from inside the container \"\"\" # Input validation if len ( numbers ) < 1 : raise ValueError ( \"Must specify at least one number to take the average of\" ) return dsl . ContainerOp ( name = \"average\" , # What will show up on the pipeline viewer image = \"k8scc01covidacr.azurecr.io/kfp-components/average:v1\" , # The image that KFP runs to do the work arguments = numbers , # Passes each number as a separate command line argument # Note that these arguments get serialized to strings file_outputs = { 'data' : './out.txt' }, # Expect an output file called out.txt to be generated # KFP can read this file and bring it back automatically ) To define our pipeline, we create a Python function decorated by the @dsl.pipeline decorator. We invoke our average_op factory to use our average container. We pass each average some inputs, and even use their outputs by accessing avg_*.output . @dsl . pipeline ( name = \"my pipeline's name\" ) def my_pipeline ( a , b , c , d , e ): \"\"\" Averaging pipeline which accepts five numbers and does some averaging operations on them \"\"\" # Compute averages for two groups avg_1 = average_op ( a , b , c ) avg_2 = average_op ( d , e ) # Use the results from _1 and _2 to compute an overall average average_result_overall = average_op ( avg_1 . output , avg_2 . output ) Finally, while we've defined our pipeline in Python, Kubeflow Pipelines itself needs everything defined as a YAML file. This final step uses the Kubeflow Pipelines Python SDK to translate our pipeline function into a YAML file that describes exactly how Kubeflow Pipelines can interact with our component. Unzip it and take a look for yourself! from kfp import compiler pipeline_yaml = 'pipeline.yaml.zip' compiler . Compiler () . compile ( my_pipeline , pipeline_yaml ) print ( f \"Exported pipeline definition to { pipeline_yaml } \" ) Kubeflow Pipelines is a lazy beast It is useful to keep in mind what computation is happening when you run this python code versus what happens when you submit the pipeline to Kubeflow Pipelines. Although it seems like everything is happening in the moment, try adding print(avg_1.output) to the above pipeline and see what happens when you compile your pipeline. The Python SDK we're using is for authoring pipelines, not for running them, so results from components will never be available when you run this Python code. The is discussed more below in Understanding what computation occurs when . To actually run our pipeline, we define an experiment: experiment_name = \"averaging-pipeline\" import kfp client = kfp . Client () exp = client . create_experiment ( name = experiment_name ) pl_params = { 'a' : 5 , 'b' : 5 , 'c' : 8 , 'd' : 10 , 'e' : 18 , } And then run an instance of our pipeline with the arguments we want: import time run = client . run_pipeline ( exp . id , # Run inside the above experiment experiment_name + '-' + time . strftime ( \"%Y%m %d -%H%M%S\" ), # Give our job a name with a timestamp so its unique pipeline_yaml , # Pass the .yaml.zip we created above. This defines the pipeline params = pl_params # Pass our parameters we want to run the pipeline with ) This can all be seen in the Kubeflow Pipelines UI : Later when we want to reuse the pipeline, we can pass different arguments and do it all again. !!! info \"We create our experiment, upload our pipeline, and run from Python in this example, but we could also do all this through the Kubeflow Pipelines UI above. Understanding what computation occurs when The above example uses Python code to define: The interface between Kubeflow Pipelines and our Docker containers doing the work (by defining ContainerOp 's) The logic of our pipeline (by defining my_pipeline ). But when we run compiler.Compiler().compile() and client.run_pipeline() , what actually happens? It is important to remember that everything we run in Python here is specifying the pipeline and its components in order to write YAML definitions, it is not doing the work of the pipeline. When running compiler.Compiler().compile() , we are not running our pipeline in the typical sense. Instead, KFP uses my_pipeline to build a YAML version of it. When we compile , the KFP SDK is passing placeholder arguments to my_pipeline and tracing where they (and any other runtime data) go, such as any output a component produces. When compile encounters a ContainerOp , nothing runs now - instead it takes note that a container will be there in future and remembers what data it will consume/generate. This can be seen by modifying and recompiling our pipeline: @dsl . pipeline ( name = \"my pipeline's name\" ) def my_pipeline ( a , b , c , d , e ): \"\"\" Averaging pipeline which accepts five numbers and does some averaging operations on them \"\"\" # NEW CODE x = 1 + 1 print ( f \"The value of x is { x } \" ) print ( f \"The value of a is { a } \" ) # Compute averages for two groups avg_1 = average_op ( a , b , c ) avg_2 = average_op ( d , e ) # NEW CODE print ( f \"The value of avg_1.output is { avg_1 . output } \" ) # Use the results from _1 and _2 to compute an overall average average_result_overall = average_op ( avg_1 . output , avg_2 . output ) And when we compile , we see print statements: The value of x is 2 The value of a is {{ pipelineparam : op = ; name = a }} The value of avg_1.output is {{ pipelineparam : op = averge ; name = data }} In the first print statement everything is normal. In the second and third print statements, however, we see string placeholders rather than actual output. So while compile does \"execute\" my_pipeline , the KFP-specific parts of the code don't actually generate results. This can also be seen in the YAML file that compile generates, for example looking at the portion defining our average_result_overall component: - name : average-3 container : args : [ \"{{inputs.parameters.average-data}}\" , \"{{inputs.parameters.average-2-data}}\" , ] image : k8scc01covidacr.azurecr.io/kfp-components/average:v1 inputs : parameters : - { name : average-2-data } - { name : average-data } outputs : artifacts : - { name : average-3-data , path : ./out.txt } metadata : labels : { pipelines.kubeflow.org/pipeline-sdk-type : kfp } In this YAML we see the input parameters passed are placeholders for data from previous components rather than their actual value. This is because while KFP knows a result from average-data and average-2-data will be passed to average, but the value of that result is not available until the pipeline is actually run. Component naming within the YAML file Because we made an average_op factory function with name='average' above, our YAML file has component names that automatically increment to avoid recreating the same name twice. We could have been fancier with our factory functions to more directly control our names, giving an argument like name='average_first_input_args' , or could even have explicitly defined the name in our pipeline by using avg_1 = average_op(a, b, c).set_display_name(\"Average 1\") . As one more example, let's try two more pipelines. One has a for loop inside which prints \"Woohoo!\" a fixed number of times. whereas the other does the same but loops n times (where n is a pipeline parameter): !!! info \"Pipeline parameters are described more below, but they work like parameters for functions. Pipelines can accept data (numbers, string URL's to large files in MinIO, etc.) as arguments, allowing a single generic pipeline to work in many situations.\" @dsl . pipeline ( name = \"my pipeline's name\" ) def another_pipeline (): \"\"\" Prints to the screen 10 times \"\"\" for i in range ( 10 ): print ( \"Woohoo!\" ) # And just so we've got some component going too... avg = average_op ( n ) compiler . Compiler () . compile ( another_pipeline , \"another.yaml.zip\" ) @dsl . pipeline ( name = \"my pipeline's name\" ) def another_another_pipeline ( n ): \"\"\" Prints to the screen n times \"\"\" for i in range ( n ): print ( \"Woohoo!\" ) # And just so we've got some component going too... avg = average_op ( n ) compiler . Compiler () . compile ( another_another_pipeline , \"another.yaml.zip\" ) The first works as you'd expect, but the second raises the exception: TypeError: 'PipelineParam' object cannot be interpreted as an integer Why? Because when authoring the pipeline n is a placeholder and has no value. KFP cannot define a pipeline from this because it does not know how many times to loop. We'd hit similar problems if using if statements. There are some ways around this, but they're left to the reader to explore through the Kubeflow Pipelines docs . Why does pipeline authoring behave this way? Because pipelines (and components ) are meant to be reusable definitions of logic that are defined in static YAML files, with all dynamic decision making done inside components. This can make them a little awkward to define, but also helps them be more reusable. Data Exchange Passing data into, within, and from a pipeline In the first example above, we pass: Numbers into our pipeline Numbers between components within our pipeline A number back to the user at the end But as discussed above, pipeline arguments and component results are just placeholder objects \u2013 so how does KFP know our values are numeric? The answer is: it doesn't. In fact, it didn't even treat them as numbers above. Instead it treated them as strings. It is just that our pipeline components worked just as well with \"5\" as they would have with 5 . A safe default assumption is that all data exchange happens as a string. When we passed a, b, ... into the pipeline, those numbers were implicitly stringified because they eventually become command line arguments for our Docker container. When we read the result of avg_1 from its out.txt , that result was read as a string. By calling average_op(avg_1.output, avg_2.output) , we ask KFP to pass the string output from avg_1 and avg_2 to a new average_op . It just so happened that, since average_op passes each string as a command line argument to our Docker image, it didn't really matter they were strings. You can still use non-string data types, but you need to pass them as serialized versions. So if we wanted our avg_1 component to return both the numbers passed to it and the average returned as a dictionary, for example: { 'numbers' : [ 5 , 5 , 8 ], 'result' : 6.0 , } We could modify our average.py in the Docker image write our dictionary of numbers and result to out.txt as JSON. But then when we pass the result to make average_result_overall , that component needs to deserialize the above JSON and pull the data from it that it needs. And because these results are not available when authoring the pipeline, something like this does not work: def my_pipeline ( a , b , c , d , e ): \"\"\" Averaging pipeline which accepts five numbers and does some averaging operations on them \"\"\" # Compute averages for two groups avg_1 = average_op_that_returns_json ( a , b , c ) avg_2 = average_op_that_returns_json ( d , e ) # THIS DOES NOT WORK! import json avg_1_result = json . loads ( avg_1 . output )[ 'result' ] avg_2_result = json . loads ( avg_2 . output )[ 'result' ] # Use the results from _1 and _2 to compute an overall average average_result_overall = average_op ( avg_1 . output , avg_2 . output ) At compile time, avg_1.output is just a placeholder and can't be treated like the JSON it will eventually become. To do something like this, we need to interpret the JSON string within a container. Parameterizing pipelines Whenever possible, create pipelines in a generic way: define parameters that might change as pipeline inputs instead of writing values directly in your Python code. For example, if you want a pipeline to process data from minimal-tenant/john-smith/data1.csv , don't hard code that path - instead, accept it as a pipeline parameter. This way you can call the same pipeline repeatedly by passing it the data location as an argument. You can see this approach in our example notebooks , where we accept MinIO credentials and the location to store our results as pipeline parameters. Passing complex/large data to/from a pipeline Although small data can often be stringified, passing by string is not suitable for complex data (large parquet files, images, etc.). It is common to use blob storage (for example: MinIO ) or other outside storage methods to persist data between components or even for later use. A typical pattern would be: Upload large/complex input data to blob storage (e.g. training data, a saved model, etc.) Pass the location of this data into the pipeline as parameters, and make your pipeline/components fetch the data as required For each component in a pipeline, specify where they place outputs in the same way For each component also return the path where it has stored its data (in this case, the string we passed it in the above bullet). This feels redundant, but it is a common pattern that lets you chain operations together Here is a schematic example of this pattern: def my_blobby_pipeline ( path_to_numbers_1 , path_to_numbers_2 , path_for_output ): \"\"\" Averaging pipeline which accepts two groups of numbers and does some averaging operations on them \"\"\" # Compute averages for two groups avg_1 = average_op_that_takes_path_to_blob ( path_to_numbers = path_to_numbers_1 , output_location = path_for_output + \"/avg_1\" ) avg_2 = average_op_that_takes_path_to_blob ( numbers = path_to_numbers_2 , output_location = path_for_output + \"/avg_2\" ) # Note that this assumes the average_op can take multiple paths to numbers. You could also have an # aggregation component that combines avg_1 and avg_2 into a single file of numbers paths_to_numbers = [ avg_1 . output , avg_2 . output ] average_result_overall = average_op ( path_to_numbers = paths_to_numbers , output_location = path_for_output + \"/average_result_overall\" ) Within this platform, the primary method for persisting large files is through MinIO as described in our Storage documentation . Examples of this are also described in our example notebooks (also found in jupyter-notebooks/self-serve-storage/ on any notebook server). Typical development patterns End-to-end pipeline development A typical pattern for building pipelines in Kubeflow Pipelines is: Define components for each of your tasks Compose your components in a @dsl.pipeline decorated function compile() your pipeline, upload your YAML files, and run This pattern lets you define portable components that can be individually tested before combining them into a full pipeline. Depending on the type and complexity of task, there are different methods for building the components. Methods for authoring components Fundamentally, every component in Kubeflow Pipelines runs a container. Kubeflow Pipelines offers several methods to define these components with different levels of flexibility and complexity. User-defined container components You can define tasks through custom Docker images. The design pattern for this is: Define (update) code for your task and commit to Git Build an image from your task (through manual command or CI pipeline) Test running this Docker image locally (and iterate if needed) Push the image to a container registry (usually Docker hub, but it will be Azure Container Registry in our case on the Advanced Analytics Workspace) Update the Kubeflow Pipeline to point to the new image (via dsl.ContainerOp like above) and test the pipeline This lets you run anything you can put into a Docker image as a task in Kubeflow Pipelines. You can manage and test your images and have complete control over how they run and what dependencies use. The docker run interface for each container becomes the API that Kubeflow Pipelines dsl.ContainerOp interacts with \u2013 running the containers is effectively like running them locally using a terminal. Anything you can make into a container with that interface can be run in Kubeflow Pipelines. !!! danger \"...however, for security reasons the platform currently does not allow users to build/run custom Docker images. This is planned for the future, but in interim see Lightweight components for a way to develop pipelines without custom images\" Lightweight Python components While full custom containers offer great flexibility, sometimes they're heavier than needed. The Kubeflow Pipelines SDK also allows for Lightweight Python Components , which are components that can be built straight from Python without building new container images for each change. These components are great for fast iteration during development, as well as for simple tasks that can be written and managed easily. This is an example of a lightweight pipeline with a single component that concatenates strings: import kfp from kfp import dsl from kfp.components import func_to_container_op def concat_string ( a , b ) -> str : return f \"( { a } | { b } )\" concat_string_component = func_to_container_op ( concat_string , base_image = \"python:3.8.3-buster\" ) @dsl . pipeline ( name = \"My lightweight pipeline\" , ) def pipeline ( str1 , str2 , str3 ): # Note that we use the concat_string_component, not the # original concat_string() function concat_result_1 = concat_string_component ( str1 , str2 ) # By using cancat_result_1's output, we define the dependency of # concat_result_2 on concat_result_1 concat_result_2 = concat_string_component ( concat_result_1 . output , str3 ) We see that our concat_string component is defined directly in Python rather than from a Docker image. In the end, our function still runs in a container, but we don't have to built it ourselves: func_to_container_op() runs our Python code inside the provided base image ( python:3.8.3-buster ). This lets use avoid building every time we change our code. The base image can be anything accessible by Kubeflow, which includes all images in the Azure Container Registry and any whitelisted images from Docker hub. Lightweight components have a number of advantages but also some drawbacks See this description of their basic characteristics, as well as this example which uses them in a more complex pipeline A convenient base image to use is the the image your notebook server is running By using the same image as your notebook server, you ensure Kubeflow Pipelines has the same packages available to it as the notebook where you do your analysis. This can help avoid errors from importing packages specific to your environment. You can find that link from the notebook server page as shown below, but make sure you prepend the registry URL (so the below image would have base_image=k8scc01covidacr.azurecr.io/machine-learning-notebook-cpu:562fa4a2899eeb9ae345c51c2491447ec31a87d7 ). Note that while using a fully featured base image for iteration is fine, it's good practice to keep production pipelines lean and only supply the necessary software. That way you reduce the startup time for each step in your pipeline. Defining components directly in YAML Components can be defined directly with a YAML file, where the designer can run terminal commands from a given Docker image. This can be a great way to make non-Python pipeline components from existing containers. As with all components, we can pass both arguments and data files into/out of the component. For example: name : Concat Strings inputs : - { name : Input text 1 , type : String , description : \"Some text to echo into a terminal and tee to a file\" , } - { name : Input text 2 , type : String , description : \"Some text to echo into a terminal and tee to a file\" , } outputs : - { name : Output filename , type : String } implementation : container : image : bash:5 command : - bash - -ex - -c - | echo \"$0 | $1\" | tee $2 - { inputValue : Input text 1 } - { inputValue : Input text 2 } - { outputPath : Output filename } This example concatenates two strings like our lightweight example above. We then define a component in python from this YAML: from kfp.components import load_component_from_file echo_and_tee = load_component_from_file ( 'path/to/echo_and_tee.yaml' ) @dsl . pipeline def my_pipeline (): echo_and_tee_task_1 = echo_and_tee ( \"My text to echo\" ) # A second use that consumes the return of the first one echo_and_tee_task_2 = echo_and_tee ( echo_and_tee_task_1 . output ) See this example for more details on using existing components. Reusing existing components Similar to well abstracted functions, well abstracted components can reduce the amount of code you have to write for any given project. For example, rather than teaching your machine learning train_model component to also save the resulting model to MinIO, you can instead have train_model return the model and then Kubeflow Pipelines can pass the model to a reusable copy_to_minio component. This reuse pattern applies to components defined through any means (containers, lightweight, or YAML). Take a look at our example notebook , which reuses provided components for simple file IO tasks.","title":"Kubeflow Pipelines"},{"location":"3-Pipelines/Kubeflow-Pipelines/#overview","text":"Kubeflow Pipelines is a platform for building machine learning workflows for deployment in a Kubernetes environment. It enables authoring pipelines that encapsulate analytical workflows (transforming data, training models, building visuals, etc.). These pipelines can be shared, reused, and scheduled, and are built to run on compute provided via Kubernetes. Here is an example of a pipeline with many sample steps feeding into a single average step. This image comes from the Kubeflow Pipelines UI In the context of the Advanced Analytics Workspace, Kubeflow Pipelines are interacted with through: The Kubeflow UI , where from the Pipelines menu you can upload pipelines, view the pipelines you have and their results, etc. The Kubeflow Pipelines python SDK , accessible through the Jupyter Notebook Servers , where you can define your components and pipelines, submit them to run now, or even save them for later. More examples in the notebooks More comprehensive pipeline examples specifically made for this platform are available on GitHub (and in every Notebook Server at /jupyter-notebooks ). You can also check out public sources . See the official Kubeflow docs for a more detailed explanation of Kubeflow Pipelines.","title":"Overview"},{"location":"3-Pipelines/Kubeflow-Pipelines/#what-are-pipelines-and-how-do-they-work","text":"A pipeline in Kubeflow Pipelines consists of one or more pipeline components chained together to form a workflow. The components are like functions, describing the individual steps in your workflow (such as pulling columns from a data store, transforming data, or training a model). The pipeline is the logic that glues components together, such as: Run Component-A Pass the output from Component-A to Component-B and Component-C ... In the above image, the logic would be running many sample steps followed by a single average step. At their core, each component has: A standalone application, packaged as a Docker image , for doing the actual work. The code in the Docker image could be a shell script, Python script, or anything else you can run from a Linux terminal, and generally will have a command line interface for data exchange (accessible through docker run ) A YAML file that describes how Kubeflow Pipelines runs this code (what Docker image should be run, what command line arguments does it accept, what output does it generate) Each component should be single purpose , modular , and reusable .","title":"What are pipelines and how do they work?"},{"location":"3-Pipelines/Kubeflow-Pipelines/#define-and-run-your-first-pipeline-using-the-python-sdk","text":"While pipelines and components are defined in Kubeflow Pipelines by YAML files that use Docker images, that does not mean we have to work directly with either YAML files or Docker images. The Kubeflow Pipelines SDK provides a way for us to define our pipeline and components directly in Python code, where the SDK then translates our Python code to YAML files for us. For our first example, let's define a simple pipeline using only the Python SDK. The purpose of this section is to give a high level view of component and pipeline authoring, not a deep dive. More detailed looks into defining your own components , passing data between components , and returning data from your pipeline are explained in more detail in further sections. The demo pipeline we define will do the following: Accept five numbers as arguments Average of the first three numbers Average of the last two numbers Average of the results of (2) and (3) To do this, we will first define our component . Our average component will call a Docker image that does the following: Accepts one or more numbers as command line arguments Returns the average of these numbers by writing them to an output file in the container (by default, to out.txt ) This Docker image is already built for us and stored in our container registry here: k8scc01covidacr.azurecr.io/kfp-components/average:v1 . Don't worry if you don't know Docker - since the image is built already, we only have to tell Kubeflow Pipelines where it is. ??? info \"Full details of the average component's Docker image are in GitHub \" This image effectively runs the following code (slightly cleaned up for brevity). By making average.py accept an arbitrary set of numbers as inputs, we can use the same average component for all steps in our pipeline : import argparse def parse_args (): parser = argparse . ArgumentParser ( description = \"Returns the average of one or \" \"more numbers as a JSON file\" ) parser . add_argument ( \"numbers\" , type = float , nargs = \"+\" , help = \"One or more numbers\" ) parser . add_argument ( \"--output_file\" , type = str , default = \"out.txt\" , help = \"Filename \" \"to write output number to\" ) return parser . parse_args () if __name__ == '__main__' : args = parse_args () numbers = args . numbers output_file = args . output_file print ( f \"Averaging numbers: {numbers}\" ) avg = sum ( numbers ) / len ( numbers ) print ( f \"Result = {avg}\" ) print ( f \"Writing output to {output_file}\" ) with open ( output_file , 'w' ) as fout : fout . write ( str ( avg )) print ( \"Done\" ) To make our average image into a Kubeflow Pipelines component , we make a kfp.dsl.ContainerOp in Python that defines how Kubeflow Pipelines interacts with our container, specifying: The Docker image location to use How to pass arguments to the running container What outputs to expect from the container We could use ContainerOp directly, but since we'll use average a few times we instead create a factory function we can reuse: from kfp import dsl def average_op ( * numbers ): \"\"\" Factory for average ContainerOps Accepts an arbitrary number of input numbers, returning a ContainerOp that passes those numbers to the underlying Docker image for averaging Returns output collected from ./out.txt from inside the container \"\"\" # Input validation if len ( numbers ) < 1 : raise ValueError ( \"Must specify at least one number to take the average of\" ) return dsl . ContainerOp ( name = \"average\" , # What will show up on the pipeline viewer image = \"k8scc01covidacr.azurecr.io/kfp-components/average:v1\" , # The image that KFP runs to do the work arguments = numbers , # Passes each number as a separate command line argument # Note that these arguments get serialized to strings file_outputs = { 'data' : './out.txt' }, # Expect an output file called out.txt to be generated # KFP can read this file and bring it back automatically ) To define our pipeline, we create a Python function decorated by the @dsl.pipeline decorator. We invoke our average_op factory to use our average container. We pass each average some inputs, and even use their outputs by accessing avg_*.output . @dsl . pipeline ( name = \"my pipeline's name\" ) def my_pipeline ( a , b , c , d , e ): \"\"\" Averaging pipeline which accepts five numbers and does some averaging operations on them \"\"\" # Compute averages for two groups avg_1 = average_op ( a , b , c ) avg_2 = average_op ( d , e ) # Use the results from _1 and _2 to compute an overall average average_result_overall = average_op ( avg_1 . output , avg_2 . output ) Finally, while we've defined our pipeline in Python, Kubeflow Pipelines itself needs everything defined as a YAML file. This final step uses the Kubeflow Pipelines Python SDK to translate our pipeline function into a YAML file that describes exactly how Kubeflow Pipelines can interact with our component. Unzip it and take a look for yourself! from kfp import compiler pipeline_yaml = 'pipeline.yaml.zip' compiler . Compiler () . compile ( my_pipeline , pipeline_yaml ) print ( f \"Exported pipeline definition to { pipeline_yaml } \" ) Kubeflow Pipelines is a lazy beast It is useful to keep in mind what computation is happening when you run this python code versus what happens when you submit the pipeline to Kubeflow Pipelines. Although it seems like everything is happening in the moment, try adding print(avg_1.output) to the above pipeline and see what happens when you compile your pipeline. The Python SDK we're using is for authoring pipelines, not for running them, so results from components will never be available when you run this Python code. The is discussed more below in Understanding what computation occurs when . To actually run our pipeline, we define an experiment: experiment_name = \"averaging-pipeline\" import kfp client = kfp . Client () exp = client . create_experiment ( name = experiment_name ) pl_params = { 'a' : 5 , 'b' : 5 , 'c' : 8 , 'd' : 10 , 'e' : 18 , } And then run an instance of our pipeline with the arguments we want: import time run = client . run_pipeline ( exp . id , # Run inside the above experiment experiment_name + '-' + time . strftime ( \"%Y%m %d -%H%M%S\" ), # Give our job a name with a timestamp so its unique pipeline_yaml , # Pass the .yaml.zip we created above. This defines the pipeline params = pl_params # Pass our parameters we want to run the pipeline with ) This can all be seen in the Kubeflow Pipelines UI : Later when we want to reuse the pipeline, we can pass different arguments and do it all again. !!! info \"We create our experiment, upload our pipeline, and run from Python in this example, but we could also do all this through the Kubeflow Pipelines UI above.","title":"Define and run your first pipeline using the Python SDK"},{"location":"3-Pipelines/Kubeflow-Pipelines/#understanding-what-computation-occurs-when","text":"The above example uses Python code to define: The interface between Kubeflow Pipelines and our Docker containers doing the work (by defining ContainerOp 's) The logic of our pipeline (by defining my_pipeline ). But when we run compiler.Compiler().compile() and client.run_pipeline() , what actually happens? It is important to remember that everything we run in Python here is specifying the pipeline and its components in order to write YAML definitions, it is not doing the work of the pipeline. When running compiler.Compiler().compile() , we are not running our pipeline in the typical sense. Instead, KFP uses my_pipeline to build a YAML version of it. When we compile , the KFP SDK is passing placeholder arguments to my_pipeline and tracing where they (and any other runtime data) go, such as any output a component produces. When compile encounters a ContainerOp , nothing runs now - instead it takes note that a container will be there in future and remembers what data it will consume/generate. This can be seen by modifying and recompiling our pipeline: @dsl . pipeline ( name = \"my pipeline's name\" ) def my_pipeline ( a , b , c , d , e ): \"\"\" Averaging pipeline which accepts five numbers and does some averaging operations on them \"\"\" # NEW CODE x = 1 + 1 print ( f \"The value of x is { x } \" ) print ( f \"The value of a is { a } \" ) # Compute averages for two groups avg_1 = average_op ( a , b , c ) avg_2 = average_op ( d , e ) # NEW CODE print ( f \"The value of avg_1.output is { avg_1 . output } \" ) # Use the results from _1 and _2 to compute an overall average average_result_overall = average_op ( avg_1 . output , avg_2 . output ) And when we compile , we see print statements: The value of x is 2 The value of a is {{ pipelineparam : op = ; name = a }} The value of avg_1.output is {{ pipelineparam : op = averge ; name = data }} In the first print statement everything is normal. In the second and third print statements, however, we see string placeholders rather than actual output. So while compile does \"execute\" my_pipeline , the KFP-specific parts of the code don't actually generate results. This can also be seen in the YAML file that compile generates, for example looking at the portion defining our average_result_overall component: - name : average-3 container : args : [ \"{{inputs.parameters.average-data}}\" , \"{{inputs.parameters.average-2-data}}\" , ] image : k8scc01covidacr.azurecr.io/kfp-components/average:v1 inputs : parameters : - { name : average-2-data } - { name : average-data } outputs : artifacts : - { name : average-3-data , path : ./out.txt } metadata : labels : { pipelines.kubeflow.org/pipeline-sdk-type : kfp } In this YAML we see the input parameters passed are placeholders for data from previous components rather than their actual value. This is because while KFP knows a result from average-data and average-2-data will be passed to average, but the value of that result is not available until the pipeline is actually run. Component naming within the YAML file Because we made an average_op factory function with name='average' above, our YAML file has component names that automatically increment to avoid recreating the same name twice. We could have been fancier with our factory functions to more directly control our names, giving an argument like name='average_first_input_args' , or could even have explicitly defined the name in our pipeline by using avg_1 = average_op(a, b, c).set_display_name(\"Average 1\") . As one more example, let's try two more pipelines. One has a for loop inside which prints \"Woohoo!\" a fixed number of times. whereas the other does the same but loops n times (where n is a pipeline parameter): !!! info \"Pipeline parameters are described more below, but they work like parameters for functions. Pipelines can accept data (numbers, string URL's to large files in MinIO, etc.) as arguments, allowing a single generic pipeline to work in many situations.\" @dsl . pipeline ( name = \"my pipeline's name\" ) def another_pipeline (): \"\"\" Prints to the screen 10 times \"\"\" for i in range ( 10 ): print ( \"Woohoo!\" ) # And just so we've got some component going too... avg = average_op ( n ) compiler . Compiler () . compile ( another_pipeline , \"another.yaml.zip\" ) @dsl . pipeline ( name = \"my pipeline's name\" ) def another_another_pipeline ( n ): \"\"\" Prints to the screen n times \"\"\" for i in range ( n ): print ( \"Woohoo!\" ) # And just so we've got some component going too... avg = average_op ( n ) compiler . Compiler () . compile ( another_another_pipeline , \"another.yaml.zip\" ) The first works as you'd expect, but the second raises the exception: TypeError: 'PipelineParam' object cannot be interpreted as an integer Why? Because when authoring the pipeline n is a placeholder and has no value. KFP cannot define a pipeline from this because it does not know how many times to loop. We'd hit similar problems if using if statements. There are some ways around this, but they're left to the reader to explore through the Kubeflow Pipelines docs . Why does pipeline authoring behave this way? Because pipelines (and components ) are meant to be reusable definitions of logic that are defined in static YAML files, with all dynamic decision making done inside components. This can make them a little awkward to define, but also helps them be more reusable.","title":"Understanding what computation occurs when"},{"location":"3-Pipelines/Kubeflow-Pipelines/#data-exchange","text":"","title":"Data Exchange"},{"location":"3-Pipelines/Kubeflow-Pipelines/#passing-data-into-within-and-from-a-pipeline","text":"In the first example above, we pass: Numbers into our pipeline Numbers between components within our pipeline A number back to the user at the end But as discussed above, pipeline arguments and component results are just placeholder objects \u2013 so how does KFP know our values are numeric? The answer is: it doesn't. In fact, it didn't even treat them as numbers above. Instead it treated them as strings. It is just that our pipeline components worked just as well with \"5\" as they would have with 5 . A safe default assumption is that all data exchange happens as a string. When we passed a, b, ... into the pipeline, those numbers were implicitly stringified because they eventually become command line arguments for our Docker container. When we read the result of avg_1 from its out.txt , that result was read as a string. By calling average_op(avg_1.output, avg_2.output) , we ask KFP to pass the string output from avg_1 and avg_2 to a new average_op . It just so happened that, since average_op passes each string as a command line argument to our Docker image, it didn't really matter they were strings. You can still use non-string data types, but you need to pass them as serialized versions. So if we wanted our avg_1 component to return both the numbers passed to it and the average returned as a dictionary, for example: { 'numbers' : [ 5 , 5 , 8 ], 'result' : 6.0 , } We could modify our average.py in the Docker image write our dictionary of numbers and result to out.txt as JSON. But then when we pass the result to make average_result_overall , that component needs to deserialize the above JSON and pull the data from it that it needs. And because these results are not available when authoring the pipeline, something like this does not work: def my_pipeline ( a , b , c , d , e ): \"\"\" Averaging pipeline which accepts five numbers and does some averaging operations on them \"\"\" # Compute averages for two groups avg_1 = average_op_that_returns_json ( a , b , c ) avg_2 = average_op_that_returns_json ( d , e ) # THIS DOES NOT WORK! import json avg_1_result = json . loads ( avg_1 . output )[ 'result' ] avg_2_result = json . loads ( avg_2 . output )[ 'result' ] # Use the results from _1 and _2 to compute an overall average average_result_overall = average_op ( avg_1 . output , avg_2 . output ) At compile time, avg_1.output is just a placeholder and can't be treated like the JSON it will eventually become. To do something like this, we need to interpret the JSON string within a container.","title":"Passing data into, within, and from a pipeline"},{"location":"3-Pipelines/Kubeflow-Pipelines/#parameterizing-pipelines","text":"Whenever possible, create pipelines in a generic way: define parameters that might change as pipeline inputs instead of writing values directly in your Python code. For example, if you want a pipeline to process data from minimal-tenant/john-smith/data1.csv , don't hard code that path - instead, accept it as a pipeline parameter. This way you can call the same pipeline repeatedly by passing it the data location as an argument. You can see this approach in our example notebooks , where we accept MinIO credentials and the location to store our results as pipeline parameters.","title":"Parameterizing pipelines"},{"location":"3-Pipelines/Kubeflow-Pipelines/#passing-complexlarge-data-tofrom-a-pipeline","text":"Although small data can often be stringified, passing by string is not suitable for complex data (large parquet files, images, etc.). It is common to use blob storage (for example: MinIO ) or other outside storage methods to persist data between components or even for later use. A typical pattern would be: Upload large/complex input data to blob storage (e.g. training data, a saved model, etc.) Pass the location of this data into the pipeline as parameters, and make your pipeline/components fetch the data as required For each component in a pipeline, specify where they place outputs in the same way For each component also return the path where it has stored its data (in this case, the string we passed it in the above bullet). This feels redundant, but it is a common pattern that lets you chain operations together Here is a schematic example of this pattern: def my_blobby_pipeline ( path_to_numbers_1 , path_to_numbers_2 , path_for_output ): \"\"\" Averaging pipeline which accepts two groups of numbers and does some averaging operations on them \"\"\" # Compute averages for two groups avg_1 = average_op_that_takes_path_to_blob ( path_to_numbers = path_to_numbers_1 , output_location = path_for_output + \"/avg_1\" ) avg_2 = average_op_that_takes_path_to_blob ( numbers = path_to_numbers_2 , output_location = path_for_output + \"/avg_2\" ) # Note that this assumes the average_op can take multiple paths to numbers. You could also have an # aggregation component that combines avg_1 and avg_2 into a single file of numbers paths_to_numbers = [ avg_1 . output , avg_2 . output ] average_result_overall = average_op ( path_to_numbers = paths_to_numbers , output_location = path_for_output + \"/average_result_overall\" ) Within this platform, the primary method for persisting large files is through MinIO as described in our Storage documentation . Examples of this are also described in our example notebooks (also found in jupyter-notebooks/self-serve-storage/ on any notebook server).","title":"Passing complex/large data to/from a pipeline"},{"location":"3-Pipelines/Kubeflow-Pipelines/#typical-development-patterns","text":"","title":"Typical development patterns"},{"location":"3-Pipelines/Kubeflow-Pipelines/#end-to-end-pipeline-development","text":"A typical pattern for building pipelines in Kubeflow Pipelines is: Define components for each of your tasks Compose your components in a @dsl.pipeline decorated function compile() your pipeline, upload your YAML files, and run This pattern lets you define portable components that can be individually tested before combining them into a full pipeline. Depending on the type and complexity of task, there are different methods for building the components.","title":"End-to-end pipeline development"},{"location":"3-Pipelines/Kubeflow-Pipelines/#methods-for-authoring-components","text":"Fundamentally, every component in Kubeflow Pipelines runs a container. Kubeflow Pipelines offers several methods to define these components with different levels of flexibility and complexity.","title":"Methods for authoring components"},{"location":"3-Pipelines/Kubeflow-Pipelines/#user-defined-container-components","text":"You can define tasks through custom Docker images. The design pattern for this is: Define (update) code for your task and commit to Git Build an image from your task (through manual command or CI pipeline) Test running this Docker image locally (and iterate if needed) Push the image to a container registry (usually Docker hub, but it will be Azure Container Registry in our case on the Advanced Analytics Workspace) Update the Kubeflow Pipeline to point to the new image (via dsl.ContainerOp like above) and test the pipeline This lets you run anything you can put into a Docker image as a task in Kubeflow Pipelines. You can manage and test your images and have complete control over how they run and what dependencies use. The docker run interface for each container becomes the API that Kubeflow Pipelines dsl.ContainerOp interacts with \u2013 running the containers is effectively like running them locally using a terminal. Anything you can make into a container with that interface can be run in Kubeflow Pipelines. !!! danger \"...however, for security reasons the platform currently does not allow users to build/run custom Docker images. This is planned for the future, but in interim see Lightweight components for a way to develop pipelines without custom images\"","title":"User-defined container components"},{"location":"3-Pipelines/Kubeflow-Pipelines/#lightweight-python-components","text":"While full custom containers offer great flexibility, sometimes they're heavier than needed. The Kubeflow Pipelines SDK also allows for Lightweight Python Components , which are components that can be built straight from Python without building new container images for each change. These components are great for fast iteration during development, as well as for simple tasks that can be written and managed easily. This is an example of a lightweight pipeline with a single component that concatenates strings: import kfp from kfp import dsl from kfp.components import func_to_container_op def concat_string ( a , b ) -> str : return f \"( { a } | { b } )\" concat_string_component = func_to_container_op ( concat_string , base_image = \"python:3.8.3-buster\" ) @dsl . pipeline ( name = \"My lightweight pipeline\" , ) def pipeline ( str1 , str2 , str3 ): # Note that we use the concat_string_component, not the # original concat_string() function concat_result_1 = concat_string_component ( str1 , str2 ) # By using cancat_result_1's output, we define the dependency of # concat_result_2 on concat_result_1 concat_result_2 = concat_string_component ( concat_result_1 . output , str3 ) We see that our concat_string component is defined directly in Python rather than from a Docker image. In the end, our function still runs in a container, but we don't have to built it ourselves: func_to_container_op() runs our Python code inside the provided base image ( python:3.8.3-buster ). This lets use avoid building every time we change our code. The base image can be anything accessible by Kubeflow, which includes all images in the Azure Container Registry and any whitelisted images from Docker hub. Lightweight components have a number of advantages but also some drawbacks See this description of their basic characteristics, as well as this example which uses them in a more complex pipeline A convenient base image to use is the the image your notebook server is running By using the same image as your notebook server, you ensure Kubeflow Pipelines has the same packages available to it as the notebook where you do your analysis. This can help avoid errors from importing packages specific to your environment. You can find that link from the notebook server page as shown below, but make sure you prepend the registry URL (so the below image would have base_image=k8scc01covidacr.azurecr.io/machine-learning-notebook-cpu:562fa4a2899eeb9ae345c51c2491447ec31a87d7 ). Note that while using a fully featured base image for iteration is fine, it's good practice to keep production pipelines lean and only supply the necessary software. That way you reduce the startup time for each step in your pipeline.","title":"Lightweight Python components"},{"location":"3-Pipelines/Kubeflow-Pipelines/#defining-components-directly-in-yaml","text":"Components can be defined directly with a YAML file, where the designer can run terminal commands from a given Docker image. This can be a great way to make non-Python pipeline components from existing containers. As with all components, we can pass both arguments and data files into/out of the component. For example: name : Concat Strings inputs : - { name : Input text 1 , type : String , description : \"Some text to echo into a terminal and tee to a file\" , } - { name : Input text 2 , type : String , description : \"Some text to echo into a terminal and tee to a file\" , } outputs : - { name : Output filename , type : String } implementation : container : image : bash:5 command : - bash - -ex - -c - | echo \"$0 | $1\" | tee $2 - { inputValue : Input text 1 } - { inputValue : Input text 2 } - { outputPath : Output filename } This example concatenates two strings like our lightweight example above. We then define a component in python from this YAML: from kfp.components import load_component_from_file echo_and_tee = load_component_from_file ( 'path/to/echo_and_tee.yaml' ) @dsl . pipeline def my_pipeline (): echo_and_tee_task_1 = echo_and_tee ( \"My text to echo\" ) # A second use that consumes the return of the first one echo_and_tee_task_2 = echo_and_tee ( echo_and_tee_task_1 . output ) See this example for more details on using existing components.","title":"Defining components directly in YAML"},{"location":"3-Pipelines/Kubeflow-Pipelines/#reusing-existing-components","text":"Similar to well abstracted functions, well abstracted components can reduce the amount of code you have to write for any given project. For example, rather than teaching your machine learning train_model component to also save the resulting model to MinIO, you can instead have train_model return the model and then Kubeflow Pipelines can pass the model to a reusable copy_to_minio component. This reuse pattern applies to components defined through any means (containers, lightweight, or YAML). Take a look at our example notebook , which reuses provided components for simple file IO tasks.","title":"Reusing existing components"}]}