{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Starting on the Advanced Analytics Workspace The Advanced Analytics Workspace portal is a great place to discover and connect to the available resources we'll be talking about here. We'll break down the standard tasks into three categories: Experimentation / Analysis Publishing Large scale production All are important, and we will address all of them, but we'll focus on the first two as these are most widely applicable. For Experiments Jupyter notebooks R , Python , and Julia Choose the CPU/RAM you need, big or small, to fit your analysis Share your workspace with your team, along with the data and notebooks within Learn More Desktops with ML-Workspace Notebooks are more easily shared than desktops, but we also have the ability to run a full desktop, with typical applications, right inside your browser. Learn More For Publishing R Shiny The platform is designed to host any kind of open source application you want. We have an R-Shiny server for hosting R-Shiny apps To create any an R-Shiny Dashboard, you just have to submit a Github Pull request to our R-Dashboards GitHub repository . For Production If an experiment turns into a product, then one of the following may be needed: Kubeflow pipelines for high-volume/intensity work Automation pipelines Ask for help in production The Advanced Analytics Workspace support staff are happy to help with production oriented use cases, and we can probably save you lots of time. Don't be shy to ask us for help ! How do I get data? How do I submit data? Every workspace can be equipped with its own storage. There are also storage buckets to publish datasets; either for internal use or for wider release. We will give an overview of the technologies here, and in the next sections there will be a more in-depth FAQ of each of them. Browse some datasets Browse some datasets here. These data sets are meant to store widely shared data. Either data that has been brought it, or data to be released out as a product. As always, ensure that the data is not sensitive.","title":"Getting Started"},{"location":"#starting-on-the-advanced-analytics-workspace","text":"The Advanced Analytics Workspace portal is a great place to discover and connect to the available resources we'll be talking about here. We'll break down the standard tasks into three categories: Experimentation / Analysis Publishing Large scale production All are important, and we will address all of them, but we'll focus on the first two as these are most widely applicable.","title":"Starting on the Advanced Analytics Workspace"},{"location":"#for-experiments","text":"","title":"For Experiments"},{"location":"#jupyter-notebooks","text":"R , Python , and Julia Choose the CPU/RAM you need, big or small, to fit your analysis Share your workspace with your team, along with the data and notebooks within Learn More","title":"Jupyter notebooks"},{"location":"#desktops-with-ml-workspace","text":"Notebooks are more easily shared than desktops, but we also have the ability to run a full desktop, with typical applications, right inside your browser. Learn More","title":"Desktops with ML-Workspace"},{"location":"#for-publishing","text":"","title":"For Publishing"},{"location":"#r-shiny","text":"The platform is designed to host any kind of open source application you want. We have an R-Shiny server for hosting R-Shiny apps To create any an R-Shiny Dashboard, you just have to submit a Github Pull request to our R-Dashboards GitHub repository .","title":"R Shiny"},{"location":"#for-production","text":"If an experiment turns into a product, then one of the following may be needed: Kubeflow pipelines for high-volume/intensity work Automation pipelines Ask for help in production The Advanced Analytics Workspace support staff are happy to help with production oriented use cases, and we can probably save you lots of time. Don't be shy to ask us for help !","title":"For Production"},{"location":"#how-do-i-get-data-how-do-i-submit-data","text":"Every workspace can be equipped with its own storage. There are also storage buckets to publish datasets; either for internal use or for wider release. We will give an overview of the technologies here, and in the next sections there will be a more in-depth FAQ of each of them. Browse some datasets Browse some datasets here. These data sets are meant to store widely shared data. Either data that has been brought it, or data to be released out as a product. As always, ensure that the data is not sensitive.","title":"How do I get data? How do I submit data?"},{"location":"Collaboration/","text":"Collaboration on the Advanced Analytics Workspace There are lots of ways to collaborate on the platform, and what's best for you depends on what you're sharing and how many people you want to share with . We can roughly break the sharable things into Data and Code , and we can share the scope of who you're sharing with No one v.s. My Team v.s. Everyone . This leads to the following table of options Private Team Statcan Code Gitlab/Github or personal folder Gitlab/Github or team folder Gitlab/Github Data personal folder or bucket team folder or bucket shared Bucket What is the difference between a bucket and a folder? Buckets are like Network Storage. See the Storage section section for more discussion of the differences between these two ideas. The way that Private v.s. Team based access is configured is with namespaces . So we start by talking about Kubeflow and Kubeflow namespaces. Then, Data and Code are better handled with slightly different tools, so we discuss the two seperately. With Data we discuss buckets and Minio , and with Code we discuss git , Gitlab , and Github . This motivates the structure of this page Team Based Collaboration (applicable to Code and Data) Sharing Code Sharing Data Team Based Collaboration What does Kubeflow do? Kubeflow runs your workspaces . You can have notebook servers (called Jupyter Servers), and in them you can create analyses in R and Python with interactive visuals. You can save and upload data, download it, and your team can work along side you. Requesting a namespace By default, everyone gets their own personal namespace, firstname-lastname . But if you want to create a namespace for a team, then there is a button to submit a request for a namespace on the portal. Click the \u22ee menu on the kubeflow section of the portal . The namespace cannot have special characters other than hypens The namespace name must only be lowercase letters a-z with dashes. Otherwise, the namespace will not be created. You will receive an email notification when the namespace is created. Once the shared namespace is created, you can access it the same as any other namespace you have through the Kubeflow UI, like shown below. You will then be able to manage the collaborators list through the kubeflow Manage Contributors tab, where you can add your colleagues to the shared namespace. To switch namespaces, take a look at the top of your window, just to the right of the Kubeflow Logo. Shared Code Teams have two options (but you can combine both!): Share a workspace in Kubeflow The advantage of sharing inside Kubeflow is that it's more free-form and it works better for .ipynb files (IPython Notebooks/Jupyter Notebooks). This method also lets you share a compute environment, so you can share resources very easily. When you share a workspace, you share A Private and Shared bucket ( /team-name and /shared/team-name ) All notebook servers in the Kubeflow Namespace Share with git, using Gitlab or Github The advantage of sharing with git is that it works with users across namespaces, and keeping code in git is a great way to manage large software projects. Don't forget to include a License! If your code is public, do not forget to keep with the Innovation Team's guidelines and use a proper License if your work is done for Statistics Canada. Recommendation: Combine both It's a great idea to always use git, and using git along with shared workspaces is a great way to combine ad-hoc sharing (through files) while also keeping your code organized and tracked. Shared Storage Sharing with your team Once you have a shared namespace, you have two shared storage approaches Storage Option Benefits Shared Jupyter Servers/Workspaces More amenable to small files, notebooks, and little experiments. Shared Buckets (see Storage ) Better suited for use in pipelines, APIs, and for large files. To learn more about the technology behind these, check out the Storage section . Sharing with Statcan In addition to private buckets, or team-shared private buckets, you can also place your files in shared storage . Within all bucket storage options ( minimal , premium , pachyderm ), you have a private bucket, and a folder inside of the shared bucket. Take a look, for instance, at the link below: shared/blair-drummond/ Any logged in user can see these files and read them freely. Sharing with the world Ask about that one in our Slack channel . There are many ways to do this from the IT side, but it's important for it to go through proper processes, so this is not done in a \"self-serve\" way that the others are. That said, it is totally possible.","title":"Collaboration"},{"location":"Collaboration/#collaboration-on-the-advanced-analytics-workspace","text":"There are lots of ways to collaborate on the platform, and what's best for you depends on what you're sharing and how many people you want to share with . We can roughly break the sharable things into Data and Code , and we can share the scope of who you're sharing with No one v.s. My Team v.s. Everyone . This leads to the following table of options Private Team Statcan Code Gitlab/Github or personal folder Gitlab/Github or team folder Gitlab/Github Data personal folder or bucket team folder or bucket shared Bucket What is the difference between a bucket and a folder? Buckets are like Network Storage. See the Storage section section for more discussion of the differences between these two ideas. The way that Private v.s. Team based access is configured is with namespaces . So we start by talking about Kubeflow and Kubeflow namespaces. Then, Data and Code are better handled with slightly different tools, so we discuss the two seperately. With Data we discuss buckets and Minio , and with Code we discuss git , Gitlab , and Github . This motivates the structure of this page Team Based Collaboration (applicable to Code and Data) Sharing Code Sharing Data","title":"Collaboration on the Advanced Analytics Workspace"},{"location":"Collaboration/#team-based-collaboration","text":"","title":"Team Based Collaboration"},{"location":"Collaboration/#what-does-kubeflow-do","text":"Kubeflow runs your workspaces . You can have notebook servers (called Jupyter Servers), and in them you can create analyses in R and Python with interactive visuals. You can save and upload data, download it, and your team can work along side you.","title":"What does Kubeflow do?"},{"location":"Collaboration/#requesting-a-namespace","text":"By default, everyone gets their own personal namespace, firstname-lastname . But if you want to create a namespace for a team, then there is a button to submit a request for a namespace on the portal. Click the \u22ee menu on the kubeflow section of the portal . The namespace cannot have special characters other than hypens The namespace name must only be lowercase letters a-z with dashes. Otherwise, the namespace will not be created. You will receive an email notification when the namespace is created. Once the shared namespace is created, you can access it the same as any other namespace you have through the Kubeflow UI, like shown below. You will then be able to manage the collaborators list through the kubeflow Manage Contributors tab, where you can add your colleagues to the shared namespace. To switch namespaces, take a look at the top of your window, just to the right of the Kubeflow Logo.","title":"Requesting a namespace"},{"location":"Collaboration/#shared-code","text":"Teams have two options (but you can combine both!):","title":"Shared Code"},{"location":"Collaboration/#share-a-workspace-in-kubeflow","text":"The advantage of sharing inside Kubeflow is that it's more free-form and it works better for .ipynb files (IPython Notebooks/Jupyter Notebooks). This method also lets you share a compute environment, so you can share resources very easily. When you share a workspace, you share A Private and Shared bucket ( /team-name and /shared/team-name ) All notebook servers in the Kubeflow Namespace","title":"Share a workspace in Kubeflow"},{"location":"Collaboration/#share-with-git-using-gitlab-or-github","text":"The advantage of sharing with git is that it works with users across namespaces, and keeping code in git is a great way to manage large software projects. Don't forget to include a License! If your code is public, do not forget to keep with the Innovation Team's guidelines and use a proper License if your work is done for Statistics Canada.","title":"Share with git, using Gitlab or Github"},{"location":"Collaboration/#recommendation-combine-both","text":"It's a great idea to always use git, and using git along with shared workspaces is a great way to combine ad-hoc sharing (through files) while also keeping your code organized and tracked.","title":"Recommendation: Combine both"},{"location":"Collaboration/#shared-storage","text":"","title":"Shared Storage"},{"location":"Collaboration/#sharing-with-your-team","text":"Once you have a shared namespace, you have two shared storage approaches Storage Option Benefits Shared Jupyter Servers/Workspaces More amenable to small files, notebooks, and little experiments. Shared Buckets (see Storage ) Better suited for use in pipelines, APIs, and for large files. To learn more about the technology behind these, check out the Storage section .","title":"Sharing with your team"},{"location":"Collaboration/#sharing-with-statcan","text":"In addition to private buckets, or team-shared private buckets, you can also place your files in shared storage . Within all bucket storage options ( minimal , premium , pachyderm ), you have a private bucket, and a folder inside of the shared bucket. Take a look, for instance, at the link below: shared/blair-drummond/ Any logged in user can see these files and read them freely.","title":"Sharing with Statcan"},{"location":"Collaboration/#sharing-with-the-world","text":"Ask about that one in our Slack channel . There are many ways to do this from the IT side, but it's important for it to go through proper processes, so this is not done in a \"self-serve\" way that the others are. That said, it is totally possible.","title":"Sharing with the world"},{"location":"Help/","text":"Have questions? or feedback? Come join us on the Advanced Analytics Workspace Slack channel ! You will find a bunch of other users of the platform there who may be able to answer your questions, and some of the engineers will usually be present on the channel. You can ask questions and provide feedback there. Slack (en) We will also post notices there if there are updates or downtime. Video tutorials After you have joined our Slack community, go and check out the following tutorials Platform official Community driven content","title":"Help/Contact"},{"location":"Help/#have-questions-or-feedback","text":"Come join us on the Advanced Analytics Workspace Slack channel ! You will find a bunch of other users of the platform there who may be able to answer your questions, and some of the engineers will usually be present on the channel. You can ask questions and provide feedback there. Slack (en) We will also post notices there if there are updates or downtime.","title":"Have questions? or feedback?"},{"location":"Help/#video-tutorials","text":"After you have joined our Slack community, go and check out the following tutorials Platform official Community driven content","title":"Video tutorials"},{"location":"Storage/","text":"Storage The platform has a range of different types of storage meant for a variety of use-cases, so this storage section applies whether you're experimenting, creating pipelines, or publishing. At the surface, there are two kinds of storage Disks (also called Volumes) Buckets (S3 or \"Blob\" storage) Disks Disks are the familiar hard drive (or SSD) style file systems! You use them directly in Kubeflow when you add workspace and data volumes to your notebook server. They are automatically mounted at the directory you choose, and serve as a reliable way to preserve your data. Even if you delete your server later, you can still remount your disks to a new one \u2013 by default they are never destroyed. This is a super simple way to store your data. And if you share a workspace with a team, your whole team can use the same server's disk just like a shared drive. Buckets Buckets are slightly more complicated, but they are good at three things: Large amounts of data Buckets can be huge: way bigger than hard drives. And they are fast. Sharing You can share files from a bucket by sharing a URL that you can get through a simple web interface. This is great for sharing data with people outside of your workspace. Programmatic Access Most importantly, it's much easier for pipelines and web browsers to access data from buckets than from a hard drive. So if you want to use pipelines, you basically have to configure them to work with a bucket. Bucket Storage We have four available types of bucket storage. Self-Serve: storage type description Minimal By default, use this one. It is HDD backed storage. Premium Use this if you need very high read/write speeds, like for training models on very large datasets. Pachyderm You will only need this if you're using Pachyderm Pipelines. Publicly Available: Public (Read-Only) Self-Serve In any of the three self-serve options, you can create a personal bucket. To log in, simply use the OpenID option seen below. Once you are logged in, you are allowed to create a personal bucket with the format firstname-lastname . Cannot yet share files from Minio with OpenID Due to an upstream bug in Minio you cannot share files yet. This will hopefully be resolved soon. In the meantime, it does work if you use your access key and secret key, which you can get from Kubeflow. Sharing You can easily share individual files. Just use the \"share\" option for a specific file and you will be provided a link that you can send to a collaborator! Programmatic Access We are currently working on letting you access your bucket storage via a folder in your notebook, but in the meantime you can access it programmatically with the command line tool mc , or via S3 API calls in R or Python. Required Kubeflow configuration If you want to enable bucket storage for your notebook, select \"Inject credentials to access MinIO object storage\" from the Configurations menu when you create your server. Otherwise, your server won't know how to sign-in to your personal storage. See the example notebooks! There is a template provided for connecting in R , python , or by the command line, provided in jupyter-notebooks/self-serve-storage . You can copy-paste and edit these examples! They should suit most of your needs. Connecting with mc To connect, simply run the following (replace FULLNAME=blair-drummond with your actual firstname-lastname ) #!/bin/sh FULLNAME = blair-drummond # Get the credentials source /vault/secrets/minio-minimal-tenant1 # Add the storage under the alias \"minio-minimal\" mc config host add minio-minimal $MINIO_URL $MINIO_ACCESS_KEY $MINIO_SECRET_KEY # Create a bucket under your name # NOTE: You can *only* create buckets named with your FIRSTNAME-LASTNAME. Any # other name will be rejected. # Private bucket (\"mb\" = \"make bucket\") mc mb minio-minimal/ ${ FULLNAME } # Shared bucket mc mb minio-minimal/shared/ ${ FULLNAME } # There you go! Now you can copy over files or folders! [ -f test.txt ] || echo \"This is a test\" > test.txt mc cp test.txt minio-minimal/ ${ FULLNAME } /test.txt Now open minimal-tenant1-minio.covid.cloud.statcan.ca , you will see your test file there! You can use mc to copy files to/from the bucket. It is very fast. You can also use mc --help to see what other options you have, like mc ls minio-minimal/FIRSTNAME-LASTNAME/ to list the contents of your bucket. Other storage options To use one of our other storage options: pachyderm or premium , simply replace minimal in the above program with the type you need.","title":"Storage"},{"location":"Storage/#storage","text":"The platform has a range of different types of storage meant for a variety of use-cases, so this storage section applies whether you're experimenting, creating pipelines, or publishing. At the surface, there are two kinds of storage Disks (also called Volumes) Buckets (S3 or \"Blob\" storage)","title":"Storage"},{"location":"Storage/#disks","text":"Disks are the familiar hard drive (or SSD) style file systems! You use them directly in Kubeflow when you add workspace and data volumes to your notebook server. They are automatically mounted at the directory you choose, and serve as a reliable way to preserve your data. Even if you delete your server later, you can still remount your disks to a new one \u2013 by default they are never destroyed. This is a super simple way to store your data. And if you share a workspace with a team, your whole team can use the same server's disk just like a shared drive.","title":"Disks"},{"location":"Storage/#buckets","text":"Buckets are slightly more complicated, but they are good at three things: Large amounts of data Buckets can be huge: way bigger than hard drives. And they are fast. Sharing You can share files from a bucket by sharing a URL that you can get through a simple web interface. This is great for sharing data with people outside of your workspace. Programmatic Access Most importantly, it's much easier for pipelines and web browsers to access data from buckets than from a hard drive. So if you want to use pipelines, you basically have to configure them to work with a bucket.","title":"Buckets"},{"location":"Storage/#bucket-storage","text":"We have four available types of bucket storage. Self-Serve: storage type description Minimal By default, use this one. It is HDD backed storage. Premium Use this if you need very high read/write speeds, like for training models on very large datasets. Pachyderm You will only need this if you're using Pachyderm Pipelines. Publicly Available: Public (Read-Only)","title":"Bucket Storage"},{"location":"Storage/#self-serve","text":"In any of the three self-serve options, you can create a personal bucket. To log in, simply use the OpenID option seen below. Once you are logged in, you are allowed to create a personal bucket with the format firstname-lastname . Cannot yet share files from Minio with OpenID Due to an upstream bug in Minio you cannot share files yet. This will hopefully be resolved soon. In the meantime, it does work if you use your access key and secret key, which you can get from Kubeflow.","title":"Self-Serve"},{"location":"Storage/#sharing","text":"You can easily share individual files. Just use the \"share\" option for a specific file and you will be provided a link that you can send to a collaborator!","title":"Sharing"},{"location":"Storage/#programmatic-access","text":"We are currently working on letting you access your bucket storage via a folder in your notebook, but in the meantime you can access it programmatically with the command line tool mc , or via S3 API calls in R or Python. Required Kubeflow configuration If you want to enable bucket storage for your notebook, select \"Inject credentials to access MinIO object storage\" from the Configurations menu when you create your server. Otherwise, your server won't know how to sign-in to your personal storage. See the example notebooks! There is a template provided for connecting in R , python , or by the command line, provided in jupyter-notebooks/self-serve-storage . You can copy-paste and edit these examples! They should suit most of your needs.","title":"Programmatic Access"},{"location":"Storage/#connecting-with-mc","text":"To connect, simply run the following (replace FULLNAME=blair-drummond with your actual firstname-lastname ) #!/bin/sh FULLNAME = blair-drummond # Get the credentials source /vault/secrets/minio-minimal-tenant1 # Add the storage under the alias \"minio-minimal\" mc config host add minio-minimal $MINIO_URL $MINIO_ACCESS_KEY $MINIO_SECRET_KEY # Create a bucket under your name # NOTE: You can *only* create buckets named with your FIRSTNAME-LASTNAME. Any # other name will be rejected. # Private bucket (\"mb\" = \"make bucket\") mc mb minio-minimal/ ${ FULLNAME } # Shared bucket mc mb minio-minimal/shared/ ${ FULLNAME } # There you go! Now you can copy over files or folders! [ -f test.txt ] || echo \"This is a test\" > test.txt mc cp test.txt minio-minimal/ ${ FULLNAME } /test.txt Now open minimal-tenant1-minio.covid.cloud.statcan.ca , you will see your test file there! You can use mc to copy files to/from the bucket. It is very fast. You can also use mc --help to see what other options you have, like mc ls minio-minimal/FIRSTNAME-LASTNAME/ to list the contents of your bucket. Other storage options To use one of our other storage options: pachyderm or premium , simply replace minimal in the above program with the type you need.","title":"Connecting with mc"},{"location":"1-Experiments/Databricks/","text":"Databricks","title":"DataBricks"},{"location":"1-Experiments/Databricks/#databricks","text":"","title":"Databricks"},{"location":"1-Experiments/Jupyter/","text":"Jupyter Friendly R and Python experience Jupyter gives you notebooks to write your code and make visualizations. You can quickly iterate, visualize, and sahre your analyses. Because it's running on a server (that you set up in the last section) you can do really big analyses on centralized hardware! Adding as much horsepower as you need! And because it's on the cloud, you can share it with your colleagues too. Explore your data Jupyter comes with a number of features (and we can add more) Integrated visuals within your notebook Data volume for storing your data You can share your workspace with colleagues. IDE in the browser Create for exploring, and also great for writing code Linting and a debugger Git integration Built in Terminal Light/Dark theme (change settings at the top) More information on Jupyter here Get started with the examples When you started your server, it got loaded with a bunch of example notebooks. Great notebooks to start with are R/01-R-Notebook-Demo.ipynb , or the notebooks in scikitlearn . pytorch and tensorflow are great if you are familiar with machine learning. The mapreduce-pipeline and ai-pipeline are more advanced. Some notebooks only work in certain server versions For instance, gdal is only in the geomatics image. So if you use another image then a notebook using gdal might not work. Adding software You do not have sudo in Jupyter, but you can use conda install --use-local your_package_name or pip install --user your_package_name Don't forget to restart your jupyter kernel afterwards, to make new packages available. Make sure to restart the Jupyter kernel after installing new software If you install software in a terminal, but your jupyter kernel was already running, then it will not be updated. Is there something that you can't install? If you need something installed, reach us or open a GitHub issue . We can add it to the default software. Getting Data in and out of Jupyter You can upload and download data to/from Jupyterhub directly in the menu. There is an upload button at the top, and you can right-click most files or folders to download them. Shareable \"Bucket\" storage The other option is high-volume storage with Object Storage . Because storage is important for experiments, publishing, and exploring datasets, it has its own section. Refer to the Storage Section","title":"Jupyter"},{"location":"1-Experiments/Jupyter/#jupyter","text":"","title":"Jupyter"},{"location":"1-Experiments/Jupyter/#friendly-r-and-python-experience","text":"Jupyter gives you notebooks to write your code and make visualizations. You can quickly iterate, visualize, and sahre your analyses. Because it's running on a server (that you set up in the last section) you can do really big analyses on centralized hardware! Adding as much horsepower as you need! And because it's on the cloud, you can share it with your colleagues too.","title":"Friendly R and Python experience"},{"location":"1-Experiments/Jupyter/#explore-your-data","text":"Jupyter comes with a number of features (and we can add more) Integrated visuals within your notebook Data volume for storing your data You can share your workspace with colleagues.","title":"Explore your data"},{"location":"1-Experiments/Jupyter/#ide-in-the-browser","text":"Create for exploring, and also great for writing code Linting and a debugger Git integration Built in Terminal Light/Dark theme (change settings at the top) More information on Jupyter here","title":"IDE in the browser"},{"location":"1-Experiments/Jupyter/#get-started-with-the-examples","text":"When you started your server, it got loaded with a bunch of example notebooks. Great notebooks to start with are R/01-R-Notebook-Demo.ipynb , or the notebooks in scikitlearn . pytorch and tensorflow are great if you are familiar with machine learning. The mapreduce-pipeline and ai-pipeline are more advanced. Some notebooks only work in certain server versions For instance, gdal is only in the geomatics image. So if you use another image then a notebook using gdal might not work.","title":"Get started with the examples"},{"location":"1-Experiments/Jupyter/#adding-software","text":"You do not have sudo in Jupyter, but you can use conda install --use-local your_package_name or pip install --user your_package_name Don't forget to restart your jupyter kernel afterwards, to make new packages available. Make sure to restart the Jupyter kernel after installing new software If you install software in a terminal, but your jupyter kernel was already running, then it will not be updated. Is there something that you can't install? If you need something installed, reach us or open a GitHub issue . We can add it to the default software.","title":"Adding software"},{"location":"1-Experiments/Jupyter/#getting-data-in-and-out-of-jupyter","text":"You can upload and download data to/from Jupyterhub directly in the menu. There is an upload button at the top, and you can right-click most files or folders to download them.","title":"Getting Data in and out of Jupyter"},{"location":"1-Experiments/Jupyter/#shareable-bucket-storage","text":"The other option is high-volume storage with Object Storage . Because storage is important for experiments, publishing, and exploring datasets, it has its own section. Refer to the Storage Section","title":"Shareable \"Bucket\" storage"},{"location":"1-Experiments/Kubeflow/","text":"Getting started with Kubeflow What does Kubeflow do? Kubeflow runs your workspaces . You can have notebook servers (called Jupyter Servers), and in them you can create analyses in R and Python with interactive visuals. You can save and upload data, download it, and create shared workspaces for your team. Let's get started! Create a Server Log into Kubeflow Log into the azure portal using your cloud.statcan credentials . Log into the Azure Portal using your Cloud Credentials You have to login to the azure portal using your statcan credentials . first.lastname@cloud.statcan.ca . You can do that using the azure portal . After logging into Azure, log into kubeflow Why am I getting \"Missing url parameter: code\"? If you try to log into kubeflow and you get the message: Missing url parameter: code It is because you are signed in with the wrong Azure account. You must sign in with your cloud credentials. Navigate to the Jupyter Servers tab Then click + New Server Configuring your server You will get a template to create your notebook server. Note: the name must be lowercase letters with hypens. No spaces, and no underscores. You'll need to choose an image You will probably want one of Machine Learning Geomatics Minimal If you want to use a gpu, check if the image says cpu or gpu . CPU and Memory At the time of writing (April 21, 2020) there are two types of computers in the cluster CPU: D16s v3 (16 vcpus, 64 GiB memory) GPU: NC6s_v3 (6 vcpus, 112 GiB memory, ? GPUs) Because of this, if you request too much RAM or too many CPUs, it may be hard or impossible to satisfy your request. In the future (possibly when you read this) there may be larger machines made available, so you may have looser restrictions. Use GPU machines responsibly There are fewer GPU machines than CPU machines, so use them responsibly. Storing your data You'll want to create a data volume! You'll be able to save your work here, and if you shut down your server, you'll be able to just remount your old data by entering the name of your old disk. It is important that you remember the volume's name. Check for old volumes by looking at the Existing option When you create your server you have the option of reusing an old volume or creating a new one. You probably want to reuse your old volume. And... Create!!! If you're satisfied with the settings, you can now create the server! It may take a few minutes to spin up depending on the resources you asked for. (GPUs take longer.) Your server is running If all goes well, your server should be running!!! You will now have the option to connect, and try out Jupyter! Share your workspace In kubeflow every user has a namespace . Your namespace belongs to you, and it's where all your resources live. If you want to collaborate with someone you need to share a namespace. So you can do that either by sharing your own namespace, or more preferably, by creating a team namespace . The link to create a new namespace is in the \u22ee menu on the kubeflow section of the portal . Manage contributors You can add or remove people from a namespace you already own through the Manage Contributors menu in kubeflow. Now you and your colleagues can share access to a server! Now you can share a server with colleagues! Try it out! For more details on collaboration on the platform, see Collaboration .","title":"Kubeflow"},{"location":"1-Experiments/Kubeflow/#getting-started-with-kubeflow","text":"","title":"Getting started with Kubeflow"},{"location":"1-Experiments/Kubeflow/#what-does-kubeflow-do","text":"Kubeflow runs your workspaces . You can have notebook servers (called Jupyter Servers), and in them you can create analyses in R and Python with interactive visuals. You can save and upload data, download it, and create shared workspaces for your team. Let's get started!","title":"What does Kubeflow do?"},{"location":"1-Experiments/Kubeflow/#create-a-server","text":"","title":"Create a Server"},{"location":"1-Experiments/Kubeflow/#log-into-kubeflow","text":"Log into the azure portal using your cloud.statcan credentials . Log into the Azure Portal using your Cloud Credentials You have to login to the azure portal using your statcan credentials . first.lastname@cloud.statcan.ca . You can do that using the azure portal . After logging into Azure, log into kubeflow Why am I getting \"Missing url parameter: code\"? If you try to log into kubeflow and you get the message: Missing url parameter: code It is because you are signed in with the wrong Azure account. You must sign in with your cloud credentials. Navigate to the Jupyter Servers tab Then click + New Server","title":"Log into Kubeflow"},{"location":"1-Experiments/Kubeflow/#configuring-your-server","text":"You will get a template to create your notebook server. Note: the name must be lowercase letters with hypens. No spaces, and no underscores. You'll need to choose an image You will probably want one of Machine Learning Geomatics Minimal If you want to use a gpu, check if the image says cpu or gpu .","title":"Configuring your server"},{"location":"1-Experiments/Kubeflow/#cpu-and-memory","text":"At the time of writing (April 21, 2020) there are two types of computers in the cluster CPU: D16s v3 (16 vcpus, 64 GiB memory) GPU: NC6s_v3 (6 vcpus, 112 GiB memory, ? GPUs) Because of this, if you request too much RAM or too many CPUs, it may be hard or impossible to satisfy your request. In the future (possibly when you read this) there may be larger machines made available, so you may have looser restrictions. Use GPU machines responsibly There are fewer GPU machines than CPU machines, so use them responsibly.","title":"CPU and Memory"},{"location":"1-Experiments/Kubeflow/#storing-your-data","text":"You'll want to create a data volume! You'll be able to save your work here, and if you shut down your server, you'll be able to just remount your old data by entering the name of your old disk. It is important that you remember the volume's name. Check for old volumes by looking at the Existing option When you create your server you have the option of reusing an old volume or creating a new one. You probably want to reuse your old volume.","title":"Storing your data"},{"location":"1-Experiments/Kubeflow/#and-create","text":"If you're satisfied with the settings, you can now create the server! It may take a few minutes to spin up depending on the resources you asked for. (GPUs take longer.) Your server is running If all goes well, your server should be running!!! You will now have the option to connect, and try out Jupyter!","title":"And... Create!!!"},{"location":"1-Experiments/Kubeflow/#share-your-workspace","text":"In kubeflow every user has a namespace . Your namespace belongs to you, and it's where all your resources live. If you want to collaborate with someone you need to share a namespace. So you can do that either by sharing your own namespace, or more preferably, by creating a team namespace . The link to create a new namespace is in the \u22ee menu on the kubeflow section of the portal .","title":"Share your workspace"},{"location":"1-Experiments/Kubeflow/#manage-contributors","text":"You can add or remove people from a namespace you already own through the Manage Contributors menu in kubeflow. Now you and your colleagues can share access to a server! Now you can share a server with colleagues! Try it out! For more details on collaboration on the platform, see Collaboration .","title":"Manage contributors"},{"location":"1-Experiments/ML-Workspaces/","text":"Ml-Workspaces How do you do it","title":"ML-Workspaces"},{"location":"1-Experiments/ML-Workspaces/#ml-workspaces","text":"How do you do it","title":"Ml-Workspaces"},{"location":"1-Experiments/MLflow/","text":"MLflow for model tracking","title":"MLflow"},{"location":"1-Experiments/MLflow/#mlflow-for-model-tracking","text":"","title":"MLflow for model tracking"},{"location":"2-Publishing/Custom/","text":"Custom Webapps We can deploy anything as long as it's open source and we can put it in a Docker container. For instance, NodeJS apps, Flask or Dash apps. Etc. See the source code for this app We just push these kinds of applications through Github into the server. The source for the above app is github.com/StatCan/covid19 How to get your app hosted If you already have a webapp in a git repository, then as soon as it's Dockerized, we can fork the Git repository into the StatCan github repository and point a url to it. To update it, you'll just interact with the Statcan Github repository with Pull Requests. Contact us if you have questions.","title":"Custom"},{"location":"2-Publishing/Custom/#custom-webapps","text":"We can deploy anything as long as it's open source and we can put it in a Docker container. For instance, NodeJS apps, Flask or Dash apps. Etc. See the source code for this app We just push these kinds of applications through Github into the server. The source for the above app is github.com/StatCan/covid19","title":"Custom Webapps"},{"location":"2-Publishing/Custom/#how-to-get-your-app-hosted","text":"If you already have a webapp in a git repository, then as soon as it's Dockerized, we can fork the Git repository into the StatCan github repository and point a url to it. To update it, you'll just interact with the Statcan Github repository with Pull Requests. Contact us if you have questions.","title":"How to get your app hosted"},{"location":"2-Publishing/PowerBI/","text":"Loading data into PowerBI We do not offer a PowerBI server, but you can pull your data into PowerBI from our Storage system, and use the data as a pandas dataframe. What you'll need A computer with PowerBI, and Python3.6 Your Minio ACCESS_KEY and SECRET_KEY on hand. (See Storage ) Get connected Set up PowerBI Open up your PowerBI system, and open up this PowerBI quickstart in your favorite text editor. You'll have to make sure that pandas , boto3 , and numpy are installed, and that you're using the right Conda Virtual Env (if applicable) You'll then need to make sure that PowerBI is using the correct Python environment. This is modified from the options menu, and the exact path is specified in the quickstart guide. Edit your python script Then, edit your Python script to use your Minio ACCESS_KEY and SECRET_KEY , and then click \"Get Data\" and copy it in as a Python Script.","title":"PowerBI"},{"location":"2-Publishing/PowerBI/#loading-data-into-powerbi","text":"We do not offer a PowerBI server, but you can pull your data into PowerBI from our Storage system, and use the data as a pandas dataframe.","title":"Loading data into PowerBI"},{"location":"2-Publishing/PowerBI/#what-youll-need","text":"A computer with PowerBI, and Python3.6 Your Minio ACCESS_KEY and SECRET_KEY on hand. (See Storage )","title":"What you'll need"},{"location":"2-Publishing/PowerBI/#get-connected","text":"","title":"Get connected"},{"location":"2-Publishing/PowerBI/#set-up-powerbi","text":"Open up your PowerBI system, and open up this PowerBI quickstart in your favorite text editor. You'll have to make sure that pandas , boto3 , and numpy are installed, and that you're using the right Conda Virtual Env (if applicable) You'll then need to make sure that PowerBI is using the correct Python environment. This is modified from the options menu, and the exact path is specified in the quickstart guide.","title":"Set up PowerBI"},{"location":"2-Publishing/PowerBI/#edit-your-python-script","text":"Then, edit your Python script to use your Minio ACCESS_KEY and SECRET_KEY , and then click \"Get Data\" and copy it in as a Python Script.","title":"Edit your python script"},{"location":"2-Publishing/R-Shiny/","text":"Deploying your R-Shiny dashboard! We handle the R-Shiny server, and it's super easy to get your dashboard onto the platform. Just send a pull request! All you have to do is send a pull request to our R-Dashboards repository . Include your repository in a folder with the name you want (for example, \"air-quality-dashboard\"). Then we will approve it and it will come online. If you need extra R libraries to be installed, send your list to the R-Shiny repository by creating a Github Issue and we will add the dependencies. See the above dashboard here The above dashboard is in Github. Take a look at the source , and see the dashboard live shiny.covid.cloud.statcan.ca/bus-dashboard Embedding dashboards into your websites Embedding dashboards in other sites We have not had a chance to look at this or prototype it yet, but if you have a use-case, feel free to reach out to engineering. We will work with you to figure something out.","title":"R Shiny"},{"location":"2-Publishing/R-Shiny/#deploying-your-r-shiny-dashboard","text":"We handle the R-Shiny server, and it's super easy to get your dashboard onto the platform.","title":"Deploying your R-Shiny dashboard!"},{"location":"2-Publishing/R-Shiny/#just-send-a-pull-request","text":"All you have to do is send a pull request to our R-Dashboards repository . Include your repository in a folder with the name you want (for example, \"air-quality-dashboard\"). Then we will approve it and it will come online. If you need extra R libraries to be installed, send your list to the R-Shiny repository by creating a Github Issue and we will add the dependencies. See the above dashboard here The above dashboard is in Github. Take a look at the source , and see the dashboard live shiny.covid.cloud.statcan.ca/bus-dashboard","title":"Just send a pull request!"},{"location":"2-Publishing/R-Shiny/#embedding-dashboards-into-your-websites","text":"Embedding dashboards in other sites We have not had a chance to look at this or prototype it yet, but if you have a use-case, feel free to reach out to engineering. We will work with you to figure something out.","title":"Embedding dashboards into your websites"},{"location":"3-Pipelines/Kubeflow-Pipelines/","text":"Overview Kubeflow Pipelines is a platform for building machine learning workflows for deployment in a Kubernetes environment. It enables authoring pipelines that encapsulate analytical workflows (transforming data, training models, building visuals, etc.). These pipelines can be shared, reused, and scheduled, and are built to run on compute provided via Kubernetes. In the context of the Advanced Analytics Workspace, Kubeflow Pipelines are interacted with through: The Kubeflow UI , where from the Pipelines menu you can upload pipelines, view the pipelines you have and their results, etc. The Kubeflow Pipelines python SDK , accessible through the Jupyter Notebook Servers , where you can define your components and pipelines, submit them to run now, or even save them for later. More examples in the notebooks More comprehensive pipeline examples specifically made for this platform are available on GitHub (and in every Notebook Server at /jupyter-notebooks ). You can also check out public sources . See the official Kubeflow docs for a more detailed explanation of Kubeflow Pipelines. What are Pipelines and How do they Work? A pipeline in Kubeflow Pipelines consists of one or more pipeline components chained together to form a workflow. The components are like functions, and then the pipeline just connects them together. The pipeline describes the entire workflow of what you want to accomplish, while the pipeline components each describe individual steps in that process (such as pulling columns from a data store, transforming data, or training a model). Each component should be modular , and ideally reusable . At their core, each component has: A standalone application, packaged as a docker images , for doing the actual work. The code in the docker image could be a shell script, python script, or anything else you can run from a Linux terminal A description of how Kubeflow Pipelines runs the code (where is the image, what command line arguments does it accept, what outputs does it generate), as a YAML file A pipeline then, using the above components , defines the logic for how components are connected, such as: run ComponentA pass the output from ComponentA to ComponentB and ComponentC ... Example of a pipeline Here's an example 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 #!/bin/python3 dsl . pipeline ( name = \"Estimate Pi\" , description = 'Estimate Pi using a Map-Reduce pattern' ) def compute_pi (): # Create a \"sample\" operation for each seed passed to the pipeline seeds = ( 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 ) sample_ops = [ sample_op ( seed ) for seed in seeds ] # Get the results, before we feed into two different pipelines. # The results are extracted from the output_file.json files, # are available from the sample_op instances through the .outputs attribute outputs = [ s . outputs [ 'output' ] for s in sample_ops ] _generate_plot_op = generate_plot_op ( outputs ) _average_op = average_op ( outputs ) You can find the full pipeline in the map-reduce-pipeline example Define and run your first Pipeline While pipelines and components are defined by YAML files, the python SDK let's you define them from python code. The following is an example of how to define a simple pipeline using the python SDK. The objective of our pipeline is, given five numbers, compute: The average of the first three numbers The average of the last two numbers The average of the results of (1) and (2) To do this, we define a pipeline that uses our average component to do the computations. The average component is defined by a docker image with a simple python script that: accepts one or more numbers as command line arguments returns the average of these numbers, written to the file out.txt within its container To tell Kubeflow Pipelines how to use this image, we define our average component through a ContainerOp which tells Kubeflow our image's API. The ContainerOp instance sets the docker image location, how to pass arguments to it, and what outputs to pull from the container. To actually use these ContainerOp's in our pipeline, we build factory functions like average_op (as we'll probably want more than just one average component ). from kfp import dsl def average_op ( * numbers ): \"\"\" Factory for average ContainerOps Accepts an arbitrary number of input numbers, returning a ContainerOp that passes those numbers to the underlying docker image for averaging Returns output collected from ./out.txt from inside the container \"\"\" # Input validation if len ( numbers ) < 1 : raise ValueError ( \"Must specify at least one number to take the average of\" ) return dsl . ContainerOp ( name = \"averge\" , # What will show up on the pipeline viewer image = \"k8scc01covidacr.azurecr.io/kfp-components/average:v1\" , # The image that KFP runs to do the work arguments = numbers , # Passes each number as a separate (string) command line argument # Script inside container writes the result (as a string) to out.txt, which # KFP reads for us and brings back here as a string file_outputs = { 'data' : './out.txt' }, ) We define our pipeline as a python function that uses our above ComponentOp factories, decorated by the @dsl.pipeline decorator. Our pipeline uses our average component by passing it numbers, and we use the average results by passing them to later functions through accessing avg_*.output . @dsl . pipeline ( name = \"my pipeline's name\" ) def my_pipeline ( a , b , c , d , e ): \"\"\" Averaging pipeline which accepts five numbers and does some averaging operations on them \"\"\" # Compute averages for two groups avg_1 = average_op ( a , b , c ) avg_2 = average_op ( d , e ) # Use the results from _1 and _2 to compute an overall average average_result_overall = average_op ( avg_1 . output , avg_2 . output ) And finally, we save a YAML definition of our pipeline for later passing to Kubeflow Pipelines. This YAML describes to Kubeflow Pipelines exactly how to run our pipeline - unzip it and take a look yourself! from kfp import compiler pipeline_yaml = 'pipeline.yaml.zip' compiler . Compiler () . compile ( my_pipeline , pipeline_yaml ) print ( f \"Exported pipeline definition to { pipeline_yaml } \" ) Kubeflow Pipelines is a lazy beast It is useful to keep in mind what computation is happening when you run this python code versus what happens when you submit the pipeline to Kubeflow Pipelines. Although it seems like everything is happening in the moment, try adding print(avg_1.output) to the above pipeline and see what happens when you compile your pipeline. The python SDK we're using is for authoring pipelines, not for running them, so results from components will never be available when you run this python code. The is discussed more below in Understanding what computation occurs when . To actually run our pipeline, we define an experiment: experiment_name = \"averaging-pipeline\" import kfp client = kfp . Client () exp = client . create_experiment ( name = experiment_name ) pl_params = { 'a' : 5 , 'b' : 5 , 'c' : 8 , 'd' : 10 , 'e' : 18 , } Which is observable in the Kubeflow Pipelines UI : And then run an instance of our pipeline with the arguments we want: import time run = client . run_pipeline ( exp . id , # Run inside the above experiment experiment_name + '-' + time . strftime ( \"%Y%m %d -%H%M%S\" ), # Give our job a name with a timestamp so its unique pipeline_yaml , # Pass the .yaml.zip we created above. This defines the pipeline params = pl_params # Pass our parameters we want to run the pipeline with ) which can also be seen in the UI: Later when we want to reuse the pipeline, we can pass different arguments and do it all again (or even reuse it from within the Kubeflow UI). To understand this example further, open it up in Kubeflow and try it for yourself. Lightweight components Under construction, sorry! Understanding what computation occurs when Under construction, sorry!","title":"Kubeflow Pipelines"},{"location":"3-Pipelines/Kubeflow-Pipelines/#overview","text":"Kubeflow Pipelines is a platform for building machine learning workflows for deployment in a Kubernetes environment. It enables authoring pipelines that encapsulate analytical workflows (transforming data, training models, building visuals, etc.). These pipelines can be shared, reused, and scheduled, and are built to run on compute provided via Kubernetes. In the context of the Advanced Analytics Workspace, Kubeflow Pipelines are interacted with through: The Kubeflow UI , where from the Pipelines menu you can upload pipelines, view the pipelines you have and their results, etc. The Kubeflow Pipelines python SDK , accessible through the Jupyter Notebook Servers , where you can define your components and pipelines, submit them to run now, or even save them for later. More examples in the notebooks More comprehensive pipeline examples specifically made for this platform are available on GitHub (and in every Notebook Server at /jupyter-notebooks ). You can also check out public sources . See the official Kubeflow docs for a more detailed explanation of Kubeflow Pipelines.","title":"Overview"},{"location":"3-Pipelines/Kubeflow-Pipelines/#what-are-pipelines-and-how-do-they-work","text":"A pipeline in Kubeflow Pipelines consists of one or more pipeline components chained together to form a workflow. The components are like functions, and then the pipeline just connects them together. The pipeline describes the entire workflow of what you want to accomplish, while the pipeline components each describe individual steps in that process (such as pulling columns from a data store, transforming data, or training a model). Each component should be modular , and ideally reusable . At their core, each component has: A standalone application, packaged as a docker images , for doing the actual work. The code in the docker image could be a shell script, python script, or anything else you can run from a Linux terminal A description of how Kubeflow Pipelines runs the code (where is the image, what command line arguments does it accept, what outputs does it generate), as a YAML file A pipeline then, using the above components , defines the logic for how components are connected, such as: run ComponentA pass the output from ComponentA to ComponentB and ComponentC ... Example of a pipeline Here's an example 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 #!/bin/python3 dsl . pipeline ( name = \"Estimate Pi\" , description = 'Estimate Pi using a Map-Reduce pattern' ) def compute_pi (): # Create a \"sample\" operation for each seed passed to the pipeline seeds = ( 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 ) sample_ops = [ sample_op ( seed ) for seed in seeds ] # Get the results, before we feed into two different pipelines. # The results are extracted from the output_file.json files, # are available from the sample_op instances through the .outputs attribute outputs = [ s . outputs [ 'output' ] for s in sample_ops ] _generate_plot_op = generate_plot_op ( outputs ) _average_op = average_op ( outputs ) You can find the full pipeline in the map-reduce-pipeline example","title":"What are Pipelines and How do they Work?"},{"location":"3-Pipelines/Kubeflow-Pipelines/#define-and-run-your-first-pipeline","text":"While pipelines and components are defined by YAML files, the python SDK let's you define them from python code. The following is an example of how to define a simple pipeline using the python SDK. The objective of our pipeline is, given five numbers, compute: The average of the first three numbers The average of the last two numbers The average of the results of (1) and (2) To do this, we define a pipeline that uses our average component to do the computations. The average component is defined by a docker image with a simple python script that: accepts one or more numbers as command line arguments returns the average of these numbers, written to the file out.txt within its container To tell Kubeflow Pipelines how to use this image, we define our average component through a ContainerOp which tells Kubeflow our image's API. The ContainerOp instance sets the docker image location, how to pass arguments to it, and what outputs to pull from the container. To actually use these ContainerOp's in our pipeline, we build factory functions like average_op (as we'll probably want more than just one average component ). from kfp import dsl def average_op ( * numbers ): \"\"\" Factory for average ContainerOps Accepts an arbitrary number of input numbers, returning a ContainerOp that passes those numbers to the underlying docker image for averaging Returns output collected from ./out.txt from inside the container \"\"\" # Input validation if len ( numbers ) < 1 : raise ValueError ( \"Must specify at least one number to take the average of\" ) return dsl . ContainerOp ( name = \"averge\" , # What will show up on the pipeline viewer image = \"k8scc01covidacr.azurecr.io/kfp-components/average:v1\" , # The image that KFP runs to do the work arguments = numbers , # Passes each number as a separate (string) command line argument # Script inside container writes the result (as a string) to out.txt, which # KFP reads for us and brings back here as a string file_outputs = { 'data' : './out.txt' }, ) We define our pipeline as a python function that uses our above ComponentOp factories, decorated by the @dsl.pipeline decorator. Our pipeline uses our average component by passing it numbers, and we use the average results by passing them to later functions through accessing avg_*.output . @dsl . pipeline ( name = \"my pipeline's name\" ) def my_pipeline ( a , b , c , d , e ): \"\"\" Averaging pipeline which accepts five numbers and does some averaging operations on them \"\"\" # Compute averages for two groups avg_1 = average_op ( a , b , c ) avg_2 = average_op ( d , e ) # Use the results from _1 and _2 to compute an overall average average_result_overall = average_op ( avg_1 . output , avg_2 . output ) And finally, we save a YAML definition of our pipeline for later passing to Kubeflow Pipelines. This YAML describes to Kubeflow Pipelines exactly how to run our pipeline - unzip it and take a look yourself! from kfp import compiler pipeline_yaml = 'pipeline.yaml.zip' compiler . Compiler () . compile ( my_pipeline , pipeline_yaml ) print ( f \"Exported pipeline definition to { pipeline_yaml } \" ) Kubeflow Pipelines is a lazy beast It is useful to keep in mind what computation is happening when you run this python code versus what happens when you submit the pipeline to Kubeflow Pipelines. Although it seems like everything is happening in the moment, try adding print(avg_1.output) to the above pipeline and see what happens when you compile your pipeline. The python SDK we're using is for authoring pipelines, not for running them, so results from components will never be available when you run this python code. The is discussed more below in Understanding what computation occurs when . To actually run our pipeline, we define an experiment: experiment_name = \"averaging-pipeline\" import kfp client = kfp . Client () exp = client . create_experiment ( name = experiment_name ) pl_params = { 'a' : 5 , 'b' : 5 , 'c' : 8 , 'd' : 10 , 'e' : 18 , } Which is observable in the Kubeflow Pipelines UI : And then run an instance of our pipeline with the arguments we want: import time run = client . run_pipeline ( exp . id , # Run inside the above experiment experiment_name + '-' + time . strftime ( \"%Y%m %d -%H%M%S\" ), # Give our job a name with a timestamp so its unique pipeline_yaml , # Pass the .yaml.zip we created above. This defines the pipeline params = pl_params # Pass our parameters we want to run the pipeline with ) which can also be seen in the UI: Later when we want to reuse the pipeline, we can pass different arguments and do it all again (or even reuse it from within the Kubeflow UI). To understand this example further, open it up in Kubeflow and try it for yourself.","title":"Define and run your first Pipeline"},{"location":"3-Pipelines/Kubeflow-Pipelines/#lightweight-components","text":"Under construction, sorry!","title":"Lightweight components"},{"location":"3-Pipelines/Kubeflow-Pipelines/#understanding-what-computation-occurs-when","text":"Under construction, sorry!","title":"Understanding what computation occurs when"},{"location":"3-Pipelines/Pachyderm/","text":"Pachyderm Check for old volumes by looking at the Existing option When you create your server you have the option of reusing an old volume or creating a new one. You probably want to reuse your old volume. Why am I getting \"Missing url parameter: code\"? If you try to log into kubeflow and you get the message Missing url parameter: code It is because you are signed in with the wrong Azure account. You must sign in with your cloud credentials.","title":"Pachyderm"},{"location":"3-Pipelines/Pachyderm/#pachyderm","text":"Check for old volumes by looking at the Existing option When you create your server you have the option of reusing an old volume or creating a new one. You probably want to reuse your old volume. Why am I getting \"Missing url parameter: code\"? If you try to log into kubeflow and you get the message Missing url parameter: code It is because you are signed in with the wrong Azure account. You must sign in with your cloud credentials.","title":"Pachyderm"}]}