{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Starting on the Advanced Analytics Workspace The Advanced Analytics Workspace portal is a great place to discover and connect to the available resources we'll be talking about here. We'll break down the standard tasks into three categories: Experimentation / Analysis Publishing Large scale production All are important, and we will address all of them, but we'll focus on the first two as these are most widely applicable. For Experiments Jupyter notebooks R , Python , and Julia Choose the CPU/RAM you need, big or small, to fit your analysis Share your workspace with your team, along with the data and notebooks within Learn More Desktops with ML-Workspace Notebooks are more easily shared than desktops, but we also have the ability to run a full desktop, with typical applications, right inside your browser. Learn More For Publishing R Shiny The platform is designed to host any kind of open source application you want. We have an R-Shiny server for hosting R-Shiny apps To create any an R-Shiny Dashboard, you just have to submit a Github Pull request to our R-Dashboards GitHub repository . For Production If an experiment turns into a product, then one of the following may be needed: Kubeflow pipelines for high-volume/intensity work Automation pipelines Ask for help in production The Advanced Analytics Workspace support staff are happy to help with production oriented use cases, and we can probably save you lots of time. Don't be shy to ask us for help ! How do I get data? How do I submit data? Every workspace can be equipped with its own storage. There are also storage buckets to publish datasets; either for internal use or for wider release. We will give an overview of the technologies here, and in the next sections there will be a more in-depth FAQ of each of them. Browse some datasets Browse some datasets here. These data sets are meant to store widely shared data. Either data that has been brought it, or data to be released out as a product. As always, ensure that the data is not sensitive.","title":"Getting Started"},{"location":"#starting-on-the-advanced-analytics-workspace","text":"The Advanced Analytics Workspace portal is a great place to discover and connect to the available resources we'll be talking about here. We'll break down the standard tasks into three categories: Experimentation / Analysis Publishing Large scale production All are important, and we will address all of them, but we'll focus on the first two as these are most widely applicable.","title":"Starting on the Advanced Analytics Workspace"},{"location":"#for-experiments","text":"","title":"For Experiments"},{"location":"#jupyter-notebooks","text":"R , Python , and Julia Choose the CPU/RAM you need, big or small, to fit your analysis Share your workspace with your team, along with the data and notebooks within Learn More","title":"Jupyter notebooks"},{"location":"#desktops-with-ml-workspace","text":"Notebooks are more easily shared than desktops, but we also have the ability to run a full desktop, with typical applications, right inside your browser. Learn More","title":"Desktops with ML-Workspace"},{"location":"#for-publishing","text":"","title":"For Publishing"},{"location":"#r-shiny","text":"The platform is designed to host any kind of open source application you want. We have an R-Shiny server for hosting R-Shiny apps To create any an R-Shiny Dashboard, you just have to submit a Github Pull request to our R-Dashboards GitHub repository .","title":"R Shiny"},{"location":"#for-production","text":"If an experiment turns into a product, then one of the following may be needed: Kubeflow pipelines for high-volume/intensity work Automation pipelines Ask for help in production The Advanced Analytics Workspace support staff are happy to help with production oriented use cases, and we can probably save you lots of time. Don't be shy to ask us for help !","title":"For Production"},{"location":"#how-do-i-get-data-how-do-i-submit-data","text":"Every workspace can be equipped with its own storage. There are also storage buckets to publish datasets; either for internal use or for wider release. We will give an overview of the technologies here, and in the next sections there will be a more in-depth FAQ of each of them. Browse some datasets Browse some datasets here. These data sets are meant to store widely shared data. Either data that has been brought it, or data to be released out as a product. As always, ensure that the data is not sensitive.","title":"How do I get data? How do I submit data?"},{"location":"Help/","text":"Ask your local engineers! TODO: Ask Brendan Feel free to contact... To report a bug","title":"Help/Contact"},{"location":"Help/#ask-your-local-engineers","text":"","title":"Ask your local engineers!"},{"location":"Help/#todo-ask-brendan","text":"Feel free to contact...","title":"TODO: Ask Brendan"},{"location":"Help/#to-report-a-bug","text":"","title":"To report a bug"},{"location":"Storage/","text":"Storage The platform has a range of different types of storage meant for a variety of use-cases, so this storage section applies whether you're experimenting, creating pipelines, or publishing. At the surface, there are two kinds of storage Disks (also called Volumes) Buckets (S3 or \"Blob\" storage) Disks Disks are the familiar Hard-drive/SDD style file systems! You can mount the disks into your Kubeflow server, and even if you delete your server, you can remount the disks, as they are never destroyed by default. This is a super simple way to store your data, and if you share a workspace with a team, your whole team can use the same server's disk just like a shared drive. Buckets Buckets are slightly more complicated, but they are good at three things: Large amounts of data Buckets can be huge. Way bigger than hard-drives. And they are fast. Sharing You can share files from a bucket by sharing a URL that you can get through a simple web interface. This is great for sharing data with people outside of your workspace. Programmatic Access Most importantly, it's much easier for pipelines and web-browsers to access data from buckets than from a hard drive. So if you want to use pipelines, you basically have to configure them to work with a bucket. Bucket Storage We have four available storage instances buckets Self-Serve Minimal Premium Pachyderm Publicly Available Public (Read-Only) Self-Serve In any of the three self-serve options, you can create a personal bucket. To log in, simply use OpenID as below Once you are logged in, you are allowed to create a personal bucket with the format firstname.lastname . Picture below. Sharing You can easily share individual files. Programmatic Access TODO: Ask Will & Zach","title":"Storage"},{"location":"Storage/#storage","text":"The platform has a range of different types of storage meant for a variety of use-cases, so this storage section applies whether you're experimenting, creating pipelines, or publishing. At the surface, there are two kinds of storage Disks (also called Volumes) Buckets (S3 or \"Blob\" storage)","title":"Storage"},{"location":"Storage/#disks","text":"Disks are the familiar Hard-drive/SDD style file systems! You can mount the disks into your Kubeflow server, and even if you delete your server, you can remount the disks, as they are never destroyed by default. This is a super simple way to store your data, and if you share a workspace with a team, your whole team can use the same server's disk just like a shared drive.","title":"Disks"},{"location":"Storage/#buckets","text":"Buckets are slightly more complicated, but they are good at three things: Large amounts of data Buckets can be huge. Way bigger than hard-drives. And they are fast. Sharing You can share files from a bucket by sharing a URL that you can get through a simple web interface. This is great for sharing data with people outside of your workspace. Programmatic Access Most importantly, it's much easier for pipelines and web-browsers to access data from buckets than from a hard drive. So if you want to use pipelines, you basically have to configure them to work with a bucket.","title":"Buckets"},{"location":"Storage/#bucket-storage","text":"We have four available storage instances buckets Self-Serve Minimal Premium Pachyderm Publicly Available Public (Read-Only)","title":"Bucket Storage"},{"location":"Storage/#self-serve","text":"In any of the three self-serve options, you can create a personal bucket. To log in, simply use OpenID as below Once you are logged in, you are allowed to create a personal bucket with the format firstname.lastname . Picture below.","title":"Self-Serve"},{"location":"Storage/#sharing","text":"You can easily share individual files.","title":"Sharing"},{"location":"Storage/#programmatic-access","text":"TODO: Ask Will & Zach","title":"Programmatic Access"},{"location":"1-Experiments/Databricks/","text":"Databricks","title":"DataBricks"},{"location":"1-Experiments/Databricks/#databricks","text":"","title":"Databricks"},{"location":"1-Experiments/Jupyter/","text":"Jupyter Friendly R and Python experience Jupyter gives you notebooks to write your code and make visualizations. You can quickly iterate, visualize, and sahre your analyses. Because it's running on a server (that you set up in the last section) you can do really big analyses on centralized hardware! Adding as much horsepower as you need! And because it's on the cloud, you can share it with your colleagues too. Explore your data Jupyter comes with a number of features (and we can add more) Integrated visuals within your notebook Data volume for storing your data You can share your workspace with colleagues. IDE in the browser Create for exploring, and also great for writing code Linting and a debugger Git integration Built in Terminal Light/Dark theme (change settings at the top) More information on Jupyter here Get started with the examples When you started your server, it got loaded with a bunch of example notebooks. Great notebooks to start with are R/01-R-Notebook-Demo.ipynb , or the notebooks in scikitlearn . pytorch and tensorflow are great if you are familiar with machine learning. The mapreduce-pipeline and ai-pipeline are more advanced. Some notebooks only work in certain server versions For instance, gdal is only in the geomatics image. So if you use another image then a notebook using gdal might not work. Adding software You do not have sudo in Jupyter, but you can use conda install --use-local your_package_name or pip install --user your_package_name Don't forget to restart your jupyter kernel afterwards, to make new packages available. Make sure to restart the Jupyter kernel after installing new software If you install software in a terminal, but your jupyter kernel was already running, then it will not be updated. Is there something that you can't install? If you need something installed, reach us or open a github issue . We can add it to the default software. Getting Data in and out of Jupyter You can upload and download data to/from Jupyterhub directly in the menu. There is an upload button at the top, and you can right-click most files or folders to download them. Shareable \"Bucket\" storage The other option is high-volume storage with Object Storage . Because storage is important for experiments, publishing, and exploring datasets, it has its own section. Refer to the Storage Section","title":"Jupyter"},{"location":"1-Experiments/Jupyter/#jupyter","text":"","title":"Jupyter"},{"location":"1-Experiments/Jupyter/#friendly-r-and-python-experience","text":"Jupyter gives you notebooks to write your code and make visualizations. You can quickly iterate, visualize, and sahre your analyses. Because it's running on a server (that you set up in the last section) you can do really big analyses on centralized hardware! Adding as much horsepower as you need! And because it's on the cloud, you can share it with your colleagues too.","title":"Friendly R and Python experience"},{"location":"1-Experiments/Jupyter/#explore-your-data","text":"Jupyter comes with a number of features (and we can add more) Integrated visuals within your notebook Data volume for storing your data You can share your workspace with colleagues.","title":"Explore your data"},{"location":"1-Experiments/Jupyter/#ide-in-the-browser","text":"Create for exploring, and also great for writing code Linting and a debugger Git integration Built in Terminal Light/Dark theme (change settings at the top) More information on Jupyter here","title":"IDE in the browser"},{"location":"1-Experiments/Jupyter/#get-started-with-the-examples","text":"When you started your server, it got loaded with a bunch of example notebooks. Great notebooks to start with are R/01-R-Notebook-Demo.ipynb , or the notebooks in scikitlearn . pytorch and tensorflow are great if you are familiar with machine learning. The mapreduce-pipeline and ai-pipeline are more advanced. Some notebooks only work in certain server versions For instance, gdal is only in the geomatics image. So if you use another image then a notebook using gdal might not work.","title":"Get started with the examples"},{"location":"1-Experiments/Jupyter/#adding-software","text":"You do not have sudo in Jupyter, but you can use conda install --use-local your_package_name or pip install --user your_package_name Don't forget to restart your jupyter kernel afterwards, to make new packages available. Make sure to restart the Jupyter kernel after installing new software If you install software in a terminal, but your jupyter kernel was already running, then it will not be updated. Is there something that you can't install? If you need something installed, reach us or open a github issue . We can add it to the default software.","title":"Adding software"},{"location":"1-Experiments/Jupyter/#getting-data-in-and-out-of-jupyter","text":"You can upload and download data to/from Jupyterhub directly in the menu. There is an upload button at the top, and you can right-click most files or folders to download them.","title":"Getting Data in and out of Jupyter"},{"location":"1-Experiments/Jupyter/#shareable-bucket-storage","text":"The other option is high-volume storage with Object Storage . Because storage is important for experiments, publishing, and exploring datasets, it has its own section. Refer to the Storage Section","title":"Shareable \"Bucket\" storage"},{"location":"1-Experiments/Kubeflow/","text":"Getting started with Kubeflow What does Kubeflow do? Kubeflow runs your workspaces . You can have notebook servers (called Jupyter Servers), and in them you can create analyses in R and Python with interactive visuals. You can save & upload data, download it, and create shared workspaces for your team. Let's get started! Create a Server Log into Kubeflow Log into the azure portal using your cloud.statcan credentials . Log into the Azure Portal using your Cloud Credentials You have to login to the azure portal using your statcan credentials . first.lastname@cloud.statcan.ca . You can do that here at the azure portal . After logging into Azure, log into kubeflow Why am I getting \"Missing url parameter: code\"? If you try to log into kubeflow and you get the message Missing url parameter: code It is because you are signed in with the wrong Azure account. You must sign in with your cloud credentials. Navigate to the Jupyter Servers tab Then click + New Server Configuring your server You will get a template to create your notebook server. Note: the name must be lowercase letters with hypens. No spaces, and no underscores. You'll need to choose an image You will probably want one of Machine Learning Geomatics Minimal If you want to use a gpu, check if the image says cpu or gpu . CPU and Memory At the time of writing (April 21, 2020) there are two types of computers in the cluster CPU: D16s v3 (16 vcpus, 64 GiB memory) GPU: NC6s_v3 (6 vcpus, 112 GiB memory, ? GPUs) Because of this, if you request too much RAM or too many CPUs, it may be hard or impossible to satisfy your request. In the future (possibly when you read this) there may be larger machines made available, so you may have looser restrictions. Use GPU machines responsibly There are fewer GPU machines than CPU machines, so use them responsibly. Storing your data You'll want to create a data volume! You'll be able to save your work here, and if you shut down your server, you'll be able to just remount your old data by entering the name of your old disk. It is important that you remember the volume's name. Check for old volumes by looking at the Existing option When you create your server you have the option of reusing an old volume or creating a new one. You probably want to reuse your old volume. And... Create!!! If you're satisfied with the settings, you can now create the server! It may take a few minutes to spin up depending on the resources you asked for. (GPUs take longer.) Your server is running If all goes well, your server should be running!!! You will now have the option to connect, and try out Jupyter! Share your workspace In kubeflow every user has a namespace . Your namespace belongs to you, and it's where all your resources live. If you want to collaborate with someone you need to share a namespace. So you can do that either by sharing your own namespace, or more preferably, by creating a team namespace . Create a new shared namespace The link to create a new namespace is here - TODO THERE IS NO LINK YET. Manage contributors You can add or remove people from a namespace you own through the Manage Contributors menu in kubeflow. Now you and your colleagues can share access to a server! Now you can share a server with colleagues! Try it out!","title":"Kubeflow"},{"location":"1-Experiments/Kubeflow/#getting-started-with-kubeflow","text":"","title":"Getting started with Kubeflow"},{"location":"1-Experiments/Kubeflow/#what-does-kubeflow-do","text":"Kubeflow runs your workspaces . You can have notebook servers (called Jupyter Servers), and in them you can create analyses in R and Python with interactive visuals. You can save & upload data, download it, and create shared workspaces for your team. Let's get started!","title":"What does Kubeflow do?"},{"location":"1-Experiments/Kubeflow/#create-a-server","text":"","title":"Create a Server"},{"location":"1-Experiments/Kubeflow/#log-into-kubeflow","text":"Log into the azure portal using your cloud.statcan credentials . Log into the Azure Portal using your Cloud Credentials You have to login to the azure portal using your statcan credentials . first.lastname@cloud.statcan.ca . You can do that here at the azure portal . After logging into Azure, log into kubeflow Why am I getting \"Missing url parameter: code\"? If you try to log into kubeflow and you get the message Missing url parameter: code It is because you are signed in with the wrong Azure account. You must sign in with your cloud credentials. Navigate to the Jupyter Servers tab Then click + New Server","title":"Log into Kubeflow"},{"location":"1-Experiments/Kubeflow/#configuring-your-server","text":"You will get a template to create your notebook server. Note: the name must be lowercase letters with hypens. No spaces, and no underscores. You'll need to choose an image You will probably want one of Machine Learning Geomatics Minimal If you want to use a gpu, check if the image says cpu or gpu .","title":"Configuring your server"},{"location":"1-Experiments/Kubeflow/#cpu-and-memory","text":"At the time of writing (April 21, 2020) there are two types of computers in the cluster CPU: D16s v3 (16 vcpus, 64 GiB memory) GPU: NC6s_v3 (6 vcpus, 112 GiB memory, ? GPUs) Because of this, if you request too much RAM or too many CPUs, it may be hard or impossible to satisfy your request. In the future (possibly when you read this) there may be larger machines made available, so you may have looser restrictions. Use GPU machines responsibly There are fewer GPU machines than CPU machines, so use them responsibly.","title":"CPU and Memory"},{"location":"1-Experiments/Kubeflow/#storing-your-data","text":"You'll want to create a data volume! You'll be able to save your work here, and if you shut down your server, you'll be able to just remount your old data by entering the name of your old disk. It is important that you remember the volume's name. Check for old volumes by looking at the Existing option When you create your server you have the option of reusing an old volume or creating a new one. You probably want to reuse your old volume.","title":"Storing your data"},{"location":"1-Experiments/Kubeflow/#and-create","text":"If you're satisfied with the settings, you can now create the server! It may take a few minutes to spin up depending on the resources you asked for. (GPUs take longer.) Your server is running If all goes well, your server should be running!!! You will now have the option to connect, and try out Jupyter!","title":"And... Create!!!"},{"location":"1-Experiments/Kubeflow/#share-your-workspace","text":"In kubeflow every user has a namespace . Your namespace belongs to you, and it's where all your resources live. If you want to collaborate with someone you need to share a namespace. So you can do that either by sharing your own namespace, or more preferably, by creating a team namespace .","title":"Share your workspace"},{"location":"1-Experiments/Kubeflow/#create-a-new-shared-namespace","text":"The link to create a new namespace is here - TODO THERE IS NO LINK YET.","title":"Create a new shared namespace"},{"location":"1-Experiments/Kubeflow/#manage-contributors","text":"You can add or remove people from a namespace you own through the Manage Contributors menu in kubeflow. Now you and your colleagues can share access to a server! Now you can share a server with colleagues! Try it out!","title":"Manage contributors"},{"location":"1-Experiments/ML-Workspaces/","text":"Ml-Workspaces How do you do it","title":"ML-Workspaces"},{"location":"1-Experiments/ML-Workspaces/#ml-workspaces","text":"How do you do it","title":"Ml-Workspaces"},{"location":"1-Experiments/MLflow/","text":"MLflow for model tracking","title":"MLflow"},{"location":"1-Experiments/MLflow/#mlflow-for-model-tracking","text":"","title":"MLflow for model tracking"},{"location":"2-Publishing/Custom/","text":"Custom Webapps We can deploy anything as long as it's open source and we can put it in a Docker container. For instance, NodeJS apps, Flask or Dash apps. Etc. See the source code for this app We just push these kinds of applications through Github into the server. The source for the above app is here github.com/StatCan/covid19 How to get your app hosted If you already have a webapp in a git repository, then as soon as it's Dockerized, we can fork the Git repository into the StatCan github repository and point a url to it. To update it, you'll just interact with the Statcan Github repository with Pull Requests. Contact us if you have questions.","title":"Custom"},{"location":"2-Publishing/Custom/#custom-webapps","text":"We can deploy anything as long as it's open source and we can put it in a Docker container. For instance, NodeJS apps, Flask or Dash apps. Etc. See the source code for this app We just push these kinds of applications through Github into the server. The source for the above app is here github.com/StatCan/covid19","title":"Custom Webapps"},{"location":"2-Publishing/Custom/#how-to-get-your-app-hosted","text":"If you already have a webapp in a git repository, then as soon as it's Dockerized, we can fork the Git repository into the StatCan github repository and point a url to it. To update it, you'll just interact with the Statcan Github repository with Pull Requests. Contact us if you have questions.","title":"How to get your app hosted"},{"location":"2-Publishing/R-Shiny/","text":"Deploying your R-Shiny dashboard! We handle the R-Shiny server, and it's super easy to get your dashboard onto the platform. Just send a pull request! All you have to do is send a pull request to our R-Dashboards repository . Include your repository in a folder with the name you want (for example, \"air-quality-dashboard\"). Then we will approve it and it will come online. If you need extra R libraries to be installed, send your list to the R-Shiny repository by creating a Github Issue and we will add the dependencies. See the above dashboard here The above dashboard is in Github. Take a look at the source , and see the dashboard live shiny.example.ca/bus-dashboard Embedding dashboards into your websites Embedding dashboards in other sites We have not had a chance to look at this or prototype it yet, but if you have a use-case, feel free to reach out to engineering. We will work with you to figure something out.","title":"R Shiny"},{"location":"2-Publishing/R-Shiny/#deploying-your-r-shiny-dashboard","text":"We handle the R-Shiny server, and it's super easy to get your dashboard onto the platform.","title":"Deploying your R-Shiny dashboard!"},{"location":"2-Publishing/R-Shiny/#just-send-a-pull-request","text":"All you have to do is send a pull request to our R-Dashboards repository . Include your repository in a folder with the name you want (for example, \"air-quality-dashboard\"). Then we will approve it and it will come online. If you need extra R libraries to be installed, send your list to the R-Shiny repository by creating a Github Issue and we will add the dependencies. See the above dashboard here The above dashboard is in Github. Take a look at the source , and see the dashboard live shiny.example.ca/bus-dashboard","title":"Just send a pull request!"},{"location":"2-Publishing/R-Shiny/#embedding-dashboards-into-your-websites","text":"Embedding dashboards in other sites We have not had a chance to look at this or prototype it yet, but if you have a use-case, feel free to reach out to engineering. We will work with you to figure something out.","title":"Embedding dashboards into your websites"},{"location":"3-Pipelines/Kubeflow-Pipelines/","text":"Experiments with Kubeflow You have to login to the azure portal using your statcan credentials . first.lastname@cloud.statcan.ca . You can do that here at the azure portal . Getting Started with notebooks","title":"Kubeflow Pipelines"},{"location":"3-Pipelines/Kubeflow-Pipelines/#experiments-with-kubeflow","text":"You have to login to the azure portal using your statcan credentials . first.lastname@cloud.statcan.ca . You can do that here at the azure portal .","title":"Experiments with Kubeflow"},{"location":"3-Pipelines/Kubeflow-Pipelines/#getting-started-with-notebooks","text":"","title":"Getting Started with notebooks"},{"location":"3-Pipelines/Pachyderm/","text":"Pachyderm Check for old volumes by looking at the Existing option When you create your server you have the option of reusing an old volume or creating a new one. You probably want to reuse your old volume. Why am I getting \"Missing url parameter: code\"? If you try to log into kubeflow and you get the message Missing url parameter: code It is because you are signed in with the wrong Azure account. You must sign in with your cloud credentials.","title":"Pachyderm"},{"location":"3-Pipelines/Pachyderm/#pachyderm","text":"Check for old volumes by looking at the Existing option When you create your server you have the option of reusing an old volume or creating a new one. You probably want to reuse your old volume. Why am I getting \"Missing url parameter: code\"? If you try to log into kubeflow and you get the message Missing url parameter: code It is because you are signed in with the wrong Azure account. You must sign in with your cloud credentials.","title":"Pachyderm"}]}